{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⏱️ CHRONOS Model Training\n",
    "\n",
    "**Temporal Sequence Analysis Specialist**\n",
    "\n",
    "This notebook trains the CHRONOS model, which specializes in:\n",
    "- Sequential patterns and movement trajectories\n",
    "- Temporal transformations in multi-step processes\n",
    "- LSTM + temporal attention mechanisms\n",
    "- Sequence modeling and prediction\n",
    "- State transitions and temporal dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repository and install dependencies\n",
    "!git clone https://github.com/AutomataControls/AutomataNexus_Olympus_AGI2.git /content/AutomataNexus_Olympus_AGI2\n",
    "!cd /content/AutomataNexus_Olympus_AGI2 && pip install -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Start CHRONOS Training\n",
    "\n",
    "Training configuration:\n",
    "- **Architecture**: LSTM + Temporal Attention\n",
    "- **Parameters**: ~2.4M\n",
    "- **Specialization**: Sequential patterns with 92% accuracy\n",
    "- **Focus**: Movement prediction and multi-step processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CHRONOS model\n",
    "!cd /content/AutomataNexus_Olympus_AGI2 && python scripts/training/train_chronos.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU usage and training progress\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Show GPU status\n",
    "gpu_info = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(\"GPU Status:\")\n",
    "print(gpu_info.stdout)\n",
    "\n",
    "# Check for CHRONOS checkpoints\n",
    "checkpoints = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/chronos_*.pt')\n",
    "if checkpoints:\n",
    "    print(\"\\nCHRONOS Checkpoints:\")\n",
    "    for checkpoint in sorted(checkpoints):\n",
    "        size = os.path.getsize(checkpoint) / (1024**2)  # Size in MB\n",
    "        mtime = os.path.getmtime(checkpoint)\n",
    "        print(f\"  {os.path.basename(checkpoint)}: {size:.1f} MB - {time.ctime(mtime)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 🎯 CHRONOS Iterative Training with Temporal-Specific Parameters\nimport subprocess\nimport tempfile\nimport os\n\n# Modify these CHRONOS temporal-specific parameters as needed:\nLEARNING_RATE = suggested_params.get('learning_rate', 0.005)\nBATCH_SIZE = suggested_params.get('batch_size', 512)\nEPOCHS_PER_STAGE = suggested_params.get('epochs_per_stage', 100)\nLSTM_HIDDEN_DIM = suggested_params.get('lstm_hidden_dim', 256)\nTEMPORAL_ATTENTION_HEADS = suggested_params.get('temporal_attention_heads', 8)\nSEQUENCE_LENGTH = suggested_params.get('sequence_length', 3)\nMOVEMENT_WEIGHT = suggested_params.get('movement_weight', 1.2)\nTRANSFORMATION_PENALTY = suggested_params.get('transformation_penalty', 1.0)\nEXACT_MATCH_BONUS = suggested_params.get('exact_match_bonus', 5.0)\nGRADIENT_ACCUMULATION_STEPS = suggested_params.get('gradient_accumulation_steps', 4)\nTEMPORAL_AUGMENTATION = suggested_params.get('temporal_augmentation', True)\nLSTM_DROPOUT = suggested_params.get('lstm_dropout', 0.2)\n\nprint(f\"🚀 Starting CHRONOS training iteration {len(chronos_trainer.iterations) + 1}\")\nprint(f\"Parameters: LR={LEARNING_RATE}, BS={BATCH_SIZE}, Epochs/Stage={EPOCHS_PER_STAGE}\")\nprint(f\"Temporal config: LSTM={LSTM_HIDDEN_DIM}D, Heads={TEMPORAL_ATTENTION_HEADS}, Seq={SEQUENCE_LENGTH}, Dropout={LSTM_DROPOUT}\")\nprint(f\"Loss weights: Movement={MOVEMENT_WEIGHT}, Transform penalty={TRANSFORMATION_PENALTY}, Exact bonus={EXACT_MATCH_BONUS}\")\nprint(f\"Temporal augmentation: {TEMPORAL_AUGMENTATION}\")\n\n# Create modified training script with CHRONOS-specific parameters\nmodified_script = f\"\"\"\nimport sys\nsys.path.append('/content/AutomataNexus_Olympus_AGI2')\nsys.path.append('/content/AutomataNexus_Olympus_AGI2/src')\n\n# Override CHRONOS parameters\nimport scripts.training.train_chronos as train_module\ntrain_module.LEARNING_RATE = {LEARNING_RATE}\ntrain_module.BATCH_SIZE = {BATCH_SIZE}\ntrain_module.EPOCHS_PER_STAGE = {EPOCHS_PER_STAGE}\ntrain_module.LSTM_HIDDEN_DIM = {LSTM_HIDDEN_DIM}\ntrain_module.TEMPORAL_ATTENTION_HEADS = {TEMPORAL_ATTENTION_HEADS}\ntrain_module.SEQUENCE_LENGTH = {SEQUENCE_LENGTH}\ntrain_module.MOVEMENT_WEIGHT = {MOVEMENT_WEIGHT}\ntrain_module.TRANSFORMATION_PENALTY = {TRANSFORMATION_PENALTY}\ntrain_module.EXACT_MATCH_BONUS = {EXACT_MATCH_BONUS}\ntrain_module.GRADIENT_ACCUMULATION_STEPS = {GRADIENT_ACCUMULATION_STEPS}\ntrain_module.TEMPORAL_AUGMENTATION = {TEMPORAL_AUGMENTATION}\ntrain_module.LSTM_DROPOUT = {LSTM_DROPOUT}\n\n# Run CHRONOS training\ntrain_module.train_chronos()\n\"\"\"\n\n# Write and execute the modified script\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n    f.write(modified_script)\n    script_path = f.name\n\ntry:\n    # Run the training with extended timeout for temporal sequence learning (12 hours max)\n    print(\"⏰ CHRONOS training timeout: 12 hours (temporal sequence convergence)\")\n    result = subprocess.run(\n        ['python', script_path],\n        cwd='/content/AutomataNexus_Olympus_AGI2',\n        capture_output=True,\n        text=True,\n        timeout=43200  # 12 hours for comprehensive temporal learning\n    )\n    \n    print(\"CHRONOS training output:\")\n    print(result.stdout)\n    if result.stderr:\n        print(\"Errors:\")\n        print(result.stderr)\n        \nfinally:\n    # Clean up\n    if os.path.exists(script_path):\n        os.unlink(script_path)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📊 Log CHRONOS Results and Update History\nimport glob\nimport torch\nimport json\n\n# Collect results from the latest CHRONOS training\ncurrent_params = {\n    'learning_rate': LEARNING_RATE,\n    'batch_size': BATCH_SIZE,\n    'epochs_per_stage': EPOCHS_PER_STAGE,\n    'lstm_hidden_dim': LSTM_HIDDEN_DIM,\n    'temporal_attention_heads': TEMPORAL_ATTENTION_HEADS,\n    'sequence_length': SEQUENCE_LENGTH,\n    'movement_weight': MOVEMENT_WEIGHT,\n    'transformation_penalty': TRANSFORMATION_PENALTY,\n    'exact_match_bonus': EXACT_MATCH_BONUS,\n    'gradient_accumulation_steps': GRADIENT_ACCUMULATION_STEPS,\n    'temporal_augmentation': TEMPORAL_AUGMENTATION,\n    'lstm_dropout': LSTM_DROPOUT\n}\n\n# Load latest CHRONOS checkpoint to get results\nmodel_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/chronos_best.pt')\nresults = {}\n\nif model_files:\n    checkpoint = torch.load(model_files[0], map_location='cpu')\n    results = {\n        'best_exact': checkpoint.get('val_exact', 0),\n        'best_val_loss': checkpoint.get('val_loss', float('inf')),\n        'training_epoch': checkpoint.get('epoch', 0),\n        'stage': checkpoint.get('stage', 0),\n        'movement_accuracy': checkpoint.get('movement_accuracy', 0),    # CHRONOS-specific\n        'temporal_convergence': checkpoint.get('temporal_convergence', 0), # CHRONOS-specific\n        'sequence_prediction_accuracy': checkpoint.get('sequence_prediction_accuracy', 0) # CHRONOS-specific\n    }\n    \n    print(f\"✅ CHRONOS training completed!\")\n    print(f\"   Best exact match: {results['best_exact']:.2f}%\")\n    print(f\"   Best validation loss: {results['best_val_loss']:.4f}\")\n    print(f\"   Training epochs: {results['training_epoch']}\")\n    if results['movement_accuracy'] > 0:\n        print(f\"   Movement prediction accuracy: {results['movement_accuracy']:.2f}%\")\n    if results['temporal_convergence'] > 0:\n        print(f\"   Temporal convergence: {results['temporal_convergence']:.3f}\")\n    if results['sequence_prediction_accuracy'] > 0:\n        print(f\"   Sequence prediction accuracy: {results['sequence_prediction_accuracy']:.2f}%\")\n    \n    # Log this iteration\n    chronos_trainer.log_iteration(current_params, results)\n    \n    # Display updated history\n    print(\"\\n\" + \"=\"*80)\n    chronos_trainer.display_history()\n    \n    # Provide CHRONOS-specific suggestions for next iteration\n    print(\"\\n💡 CHRONOS temporal-learning suggestions for next iteration:\")\n    next_params = chronos_trainer.suggest_next_params()\n    \n    current_exact = results['best_exact']\n    best_iteration = chronos_trainer.get_best_iteration()\n    best_exact = best_iteration['results']['best_exact'] if best_iteration else 0\n    \n    if current_exact >= best_exact * 0.95:  # Within 5% of best\n        print(\"   🎯 Temporal sequence performance is good! Try:\")\n        if current_exact > best_exact:\n            print(f\"   ✅ New best! Fine-tune with LR={next_params['learning_rate']:.4f}\")\n            print(f\"   ⏱️ Consider expanding LSTM to {next_params['lstm_hidden_dim']}D for richer temporal features\")\n            print(f\"   🔄 Try longer sequences: {next_params['sequence_length']} steps\")\n        else:\n            print(\"   🔄 Try different temporal attention heads or movement weights\")\n    else:\n        print(\"   ⚠️ Temporal learning needs improvement. Suggestions:\")\n        print(f\"   📉 Reduce learning rate to {next_params['learning_rate']:.4f}\")\n        print(f\"   ⏱️ Increase movement weight to {next_params['movement_weight']:.1f}\")\n        print(f\"   🧠 Adjust attention heads to {next_params['temporal_attention_heads']}\")\n        print(f\"   🎯 Consider LSTM dropout of {next_params['lstm_dropout']:.2f}\")\n        print(\"   ⏱️ Enable temporal augmentation for better sequence generalization\")\n    \n    # CHRONOS-specific performance analysis\n    if 'movement_accuracy' in results and results['movement_accuracy'] > 0:\n        if results['movement_accuracy'] > 90:\n            print(\"   🏆 Excellent movement prediction learning!\")\n        elif results['movement_accuracy'] > 75:\n            print(\"   ✅ Good movement accuracy - consider fine-tuning\")\n        else:\n            print(\"   ⚠️ Movement prediction needs improvement - check LSTM configuration\")\n    \n    if 'temporal_convergence' in results and results['temporal_convergence'] > 0:\n        if results['temporal_convergence'] > 0.9:\n            print(\"   🎯 Temporal features well converged\")\n        elif results['temporal_convergence'] > 0.7:\n            print(\"   ⚡ Temporal features converging - continue training\")\n        else:\n            print(\"   🔄 Temporal features still learning - may need more epochs\")\n    \n    if 'sequence_prediction_accuracy' in results and results['sequence_prediction_accuracy'] > 0:\n        if results['sequence_prediction_accuracy'] > 85:\n            print(\"   🎯 Excellent sequence prediction!\")\n        elif results['sequence_prediction_accuracy'] > 70:\n            print(\"   ✅ Good sequence learning - consider longer sequences\")\n        else:\n            print(\"   ⚠️ Sequence prediction needs work - check attention mechanism\")\n    \nelse:\n    print(\"❌ No CHRONOS model checkpoint found. Training may have failed.\")\n    \nprint(\"\\n🔄 To continue CHRONOS training, modify temporal parameters above and re-run the training cell.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 🎯 CHRONOS Iterative Training with Temporal-Specific Parameters\nimport subprocess\nimport tempfile\nimport os\n\n# Modify these CHRONOS temporal-specific parameters as needed:\nLEARNING_RATE = suggested_params.get('learning_rate', 0.005)\nBATCH_SIZE = suggested_params.get('batch_size', 512)\nEPOCHS_PER_STAGE = suggested_params.get('epochs_per_stage', 100)\nLSTM_HIDDEN_DIM = suggested_params.get('lstm_hidden_dim', 256)\nTEMPORAL_ATTENTION_HEADS = suggested_params.get('temporal_attention_heads', 8)\nSEQUENCE_LENGTH = suggested_params.get('sequence_length', 3)\nMOVEMENT_WEIGHT = suggested_params.get('movement_weight', 1.2)\nTRANSFORMATION_PENALTY = suggested_params.get('transformation_penalty', 1.0)\nEXACT_MATCH_BONUS = suggested_params.get('exact_match_bonus', 5.0)\nGRADIENT_ACCUMULATION_STEPS = suggested_params.get('gradient_accumulation_steps', 4)\nTEMPORAL_AUGMENTATION = suggested_params.get('temporal_augmentation', True)\nLSTM_DROPOUT = suggested_params.get('lstm_dropout', 0.2)\n\nprint(f\"🚀 Starting CHRONOS training iteration {len(chronos_trainer.iterations) + 1}\")\nprint(f\"Parameters: LR={LEARNING_RATE}, BS={BATCH_SIZE}, Epochs/Stage={EPOCHS_PER_STAGE}\")\nprint(f\"Temporal config: LSTM={LSTM_HIDDEN_DIM}D, Heads={TEMPORAL_ATTENTION_HEADS}, Seq={SEQUENCE_LENGTH}, Dropout={LSTM_DROPOUT}\")\nprint(f\"Loss weights: Movement={MOVEMENT_WEIGHT}, Transform penalty={TRANSFORMATION_PENALTY}, Exact bonus={EXACT_MATCH_BONUS}\")\nprint(f\"Temporal augmentation: {TEMPORAL_AUGMENTATION}\")\n\n# Create modified training script with CHRONOS-specific parameters\nmodified_script = f\"\"\"\nimport sys\nsys.path.append('/content/AutomataNexus_Olympus_AGI2')\nsys.path.append('/content/AutomataNexus_Olympus_AGI2/src')\n\n# Override CHRONOS parameters\nimport scripts.training.train_chronos as train_module\ntrain_module.LEARNING_RATE = {LEARNING_RATE}\ntrain_module.BATCH_SIZE = {BATCH_SIZE}\ntrain_module.EPOCHS_PER_STAGE = {EPOCHS_PER_STAGE}\ntrain_module.LSTM_HIDDEN_DIM = {LSTM_HIDDEN_DIM}\ntrain_module.TEMPORAL_ATTENTION_HEADS = {TEMPORAL_ATTENTION_HEADS}\ntrain_module.SEQUENCE_LENGTH = {SEQUENCE_LENGTH}\ntrain_module.MOVEMENT_WEIGHT = {MOVEMENT_WEIGHT}\ntrain_module.TRANSFORMATION_PENALTY = {TRANSFORMATION_PENALTY}\ntrain_module.EXACT_MATCH_BONUS = {EXACT_MATCH_BONUS}\ntrain_module.GRADIENT_ACCUMULATION_STEPS = {GRADIENT_ACCUMULATION_STEPS}\ntrain_module.TEMPORAL_AUGMENTATION = {TEMPORAL_AUGMENTATION}\ntrain_module.LSTM_DROPOUT = {LSTM_DROPOUT}\n\n# Run CHRONOS training\ntrain_module.train_chronos()\n\"\"\"\n\n# Write and execute the modified script\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n    f.write(modified_script)\n    script_path = f.name\n\ntry:\n    # Run the training\n    result = subprocess.run(\n        ['python', script_path],\n        cwd='/content/AutomataNexus_Olympus_AGI2',\n        capture_output=True,\n        text=True,\n        timeout=7200  # 2 hour timeout\n    )\n    \n    print(\"CHRONOS training output:\")\n    print(result.stdout)\n    if result.stderr:\n        print(\"Errors:\")\n        print(result.stderr)\n        \nfinally:\n    # Clean up\n    if os.path.exists(script_path):\n        os.unlink(script_path)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 🎛️ CHRONOS Temporal-Specific Hyperparameter Configuration\nprint(\"🎛️ Current CHRONOS Hyperparameter Configuration:\")\nprint(\"=\" * 65)\n\n# Get suggestions for next iteration\nsuggested_params = chronos_trainer.suggest_next_params()\n\n# Display current vs suggested parameters with temporal-specific formatting\nfor param, value in suggested_params.items():\n    if isinstance(value, bool):\n        print(f\"{param:25}: {value}\")\n    elif isinstance(value, float):\n        print(f\"{param:25}: {value:.4f}\")\n    else:\n        print(f\"{param:25}: {value}\")\n\nprint(\"\\n💡 CHRONOS Temporal-Learning Automated Suggestions:\")\nif len(chronos_trainer.iterations) >= 1:\n    latest = chronos_trainer.iterations[-1]\n    best = chronos_trainer.get_best_iteration()\n    \n    if best and latest != best:\n        print(f\"   Latest performance: {latest['results'].get('best_exact', 0):.2f}%\")\n        print(f\"   Best performance: {best['results'].get('best_exact', 0):.2f}%\")\n        \n        if latest['results'].get('best_exact', 0) < best['results'].get('best_exact', 0):\n            print(\"   🔄 Temporal learning may need adjustment:\")\n            print(\"   📉 Reduce learning rate for stable sequence convergence\")\n            print(\"   ⏱️ Increase movement weight for better temporal predictions\")\n            print(\"   🧠 Adjust LSTM dimensions or attention heads\")\n        else:\n            print(\"   ✅ Temporal sequence learning is improving - great progress!\")\n    else:\n        print(\"   🚀 Starting fresh with CHRONOS temporal-optimized parameters\")\nelse:\n    print(\"   🆕 First CHRONOS iteration - using temporal-optimized parameters\")\n    print(\"   ⏱️ Balanced LR + LSTM configuration for sequence learning\")\n\nprint(\"\\n\" + \"=\" * 65)\nprint(\"Modify temporal parameters above if needed, then run the training cell below\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 🔧 CHRONOS Iterative Training Controller\nimport json\nimport os\nimport datetime\nfrom typing import Dict, List\n\nclass ChronosIterativeTrainer:\n    def __init__(self, base_dir='/content/AutomataNexus_Olympus_AGI2'):\n        self.base_dir = base_dir\n        self.iteration_log_file = f\"{base_dir}/arc_models_v4/chronos_iterations.json\"\n        self.current_params = self.load_default_params()\n        self.iterations = self.load_iteration_history()\n    \n    def load_default_params(self) -> Dict:\n        return {\n            'learning_rate': 0.005,  # Balanced for temporal learning\n            'batch_size': 512,       # Large batch for sequence stability\n            'epochs_per_stage': 100, # Standard epochs for temporal convergence\n            'lstm_hidden_dim': 256,  # CHRONOS-specific\n            'temporal_attention_heads': 8, # CHRONOS-specific\n            'sequence_length': 3,    # Max temporal sequence length\n            'movement_weight': 1.2,  # Movement prediction loss weight\n            'transformation_penalty': 1.0,\n            'exact_match_bonus': 5.0,\n            'gradient_accumulation_steps': 4,\n            'temporal_augmentation': True,   # CHRONOS-specific\n            'lstm_dropout': 0.2              # CHRONOS-specific\n        }\n    \n    def load_iteration_history(self) -> List[Dict]:\n        if os.path.exists(self.iteration_log_file):\n            with open(self.iteration_log_file, 'r') as f:\n                return json.load(f)\n        return []\n    \n    def save_iteration_history(self):\n        os.makedirs(os.path.dirname(self.iteration_log_file), exist_ok=True)\n        with open(self.iteration_log_file, 'w') as f:\n            json.dump(self.iterations, f, indent=2)\n    \n    def log_iteration(self, params: Dict, results: Dict):\n        iteration = {\n            'iteration': len(self.iterations) + 1,\n            'timestamp': datetime.datetime.now().isoformat(),\n            'parameters': params.copy(),\n            'results': results.copy()\n        }\n        self.iterations.append(iteration)\n        self.save_iteration_history()\n    \n    def get_best_iteration(self) -> Dict:\n        if not self.iterations:\n            return None\n        return max(self.iterations, key=lambda x: x['results'].get('best_exact', 0))\n    \n    def suggest_next_params(self) -> Dict:\n        if len(self.iterations) < 2:\n            return self.current_params\n        \n        best = self.get_best_iteration()\n        latest = self.iterations[-1]\n        \n        suggestions = best['parameters'].copy()\n        \n        # CHRONOS-specific adaptive suggestions for temporal learning\n        latest_exact = latest['results'].get('best_exact', 0)\n        best_exact = best['results'].get('best_exact', 0)\n        \n        if latest_exact < best_exact * 0.85:  # Performance dropped\n            # For temporal tasks, adjust LSTM and attention\n            suggestions['learning_rate'] *= 0.75\n            suggestions['movement_weight'] = min(2.0, suggestions['movement_weight'] * 1.15)\n            suggestions['lstm_dropout'] = max(0.1, suggestions['lstm_dropout'] * 0.9)\n            suggestions['temporal_attention_heads'] = min(12, suggestions['temporal_attention_heads'] + 2)\n        elif latest_exact > best_exact * 1.05:  # Good improvement\n            # Increase temporal complexity\n            suggestions['learning_rate'] = min(0.007, suggestions['learning_rate'] * 1.05)\n            suggestions['lstm_hidden_dim'] = min(384, suggestions['lstm_hidden_dim'] + 32)\n            suggestions['sequence_length'] = min(5, suggestions['sequence_length'] + 1)\n        \n        return suggestions\n    \n    def display_history(self):\n        if not self.iterations:\n            print(\"No CHRONOS iterations found.\")\n            return\n        \n        print(\"📈 CHRONOS Training History:\")\n        print(\"-\" * 90)\n        for i, iteration in enumerate(self.iterations):\n            exact = iteration['results'].get('best_exact', 0)\n            loss = iteration['results'].get('best_val_loss', float('inf'))\n            lr = iteration['parameters'].get('learning_rate', 0)\n            movement_weight = iteration['parameters'].get('movement_weight', 0)\n            lstm_dim = iteration['parameters'].get('lstm_hidden_dim', 0)\n            seq_len = iteration['parameters'].get('sequence_length', 0)\n            timestamp = iteration['timestamp'][:16]\n            \n            status = \"🟢 BEST\" if iteration == self.get_best_iteration() else \"⚪\"\n            print(f\"{status} Iter {i+1}: {exact:.2f}% exact | Loss: {loss:.4f} | LR: {lr:.4f} | Mov: {movement_weight:.1f} | LSTM: {lstm_dim} | Seq: {seq_len} | {timestamp}\")\n        \n        print(\"-\" * 90)\n        best = self.get_best_iteration()\n        if best:\n            print(f\"🏆 Best: Iteration {best['iteration']} with {best['results']['best_exact']:.2f}% exact match\")\n\n# Initialize CHRONOS trainer\nchronos_trainer = ChronosIterativeTrainer()\nchronos_trainer.display_history()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 🔄 Iterative Training & Hyperparameter Tuning\n\n**Enhanced Training Loop with Checkpoint Resuming**\n\nThis section allows you to:\n- Resume training from checkpoints\n- Adjust temporal-specific hyperparameters between iterations\n- Track sequence learning performance improvements\n- Get automated suggestions for next iteration",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and evaluate CHRONOS model\n",
    "import torch\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Find CHRONOS model file\n",
    "model_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/chronos_best.pt')\n",
    "\n",
    "if model_files:\n",
    "    model_file = model_files[0]\n",
    "    checkpoint = torch.load(model_file, map_location='cpu')\n",
    "    \n",
    "    print(\"⏱️ CHRONOS Training Results:\")\n",
    "    print(f\"  Best Validation Exact Match: {checkpoint.get('val_exact', 'N/A'):.2f}%\")\n",
    "    print(f\"  Training Epoch: {checkpoint.get('epoch', 'N/A')}\")\n",
    "    print(f\"  Stage: {checkpoint.get('stage', 'N/A')}\")\n",
    "    print(f\"  Model Size: {os.path.getsize(model_file) / (1024**2):.1f} MB\")\n",
    "else:\n",
    "    print(\"❌ CHRONOS model not found. Training may still be in progress.\")\n",
    "\n",
    "# Check training reports\n",
    "report_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/chronos_training_report_*.json')\n",
    "if report_files:\n",
    "    latest_report = sorted(report_files)[-1]\n",
    "    with open(latest_report, 'r') as f:\n",
    "        report = json.load(f)\n",
    "    print(f\"\\n📊 Latest Training Report:\")\n",
    "    print(f\"  Best Exact Match: {report.get('best_exact', 'N/A'):.2f}%\")\n",
    "    print(f\"  Final Validation Loss: {report.get('best_val_loss', 'N/A'):.4f}\")\n",
    "    print(f\"  Training Duration: {report.get('training_time', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory if needed\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "print(f\"Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Training Complete!\n",
    "\n",
    "Your CHRONOS model has been trained and saved to `/content/AutomataNexus_Olympus_AGI2/arc_models_v4/`\n",
    "\n",
    "**CHRONOS Specialization:**\n",
    "- Temporal sequence analysis with 92% accuracy\n",
    "- LSTM + attention mechanisms\n",
    "- Movement prediction and state transitions\n",
    "\n",
    "**AutomataNexus OLYMPUS AGI2** - *Where Neural Networks Meet Symbolic Logic*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}