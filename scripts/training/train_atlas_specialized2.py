"""
ATLAS Specialized Training Script V2 - Enhanced Spatial Transformation Expert
Builds upon train_atlas_specialized.py with targeted improvements
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from torch.amp import GradScaler, autocast
import numpy as np
import json
import os
import sys
import gc
import time
from tqdm import tqdm
from typing import Dict, List, Optional
import random
from collections import defaultdict

# Add project paths
sys.path.append('/content/AutomataNexus_Olympus_AGI2')
sys.path.append('/content/AutomataNexus_Olympus_AGI2/src')
sys.path.append('/content/AutomataNexus_Olympus_AGI2/scripts/training')

# Import ATLAS model
from src.models.atlas_model import EnhancedAtlasNet

# Import ALL AutomataNexus novel training components
from src.dsl import DSLTrainingIntegration, DSLProgramGenerator
from src.dsl.atlas_dsl import ATLASDSLTraining, ATLASDSLGenerator
from src.program_synthesis.synthesis_integration import LightweightProgramSynthesizer, ProgramSynthesisDataGenerator

# ATLAS-specific program synthesis
try:
    from src.program_synthesis.atlas_synthesis import ATLASProgramSynthesizer, create_atlas_synthesis_system
    ATLAS_SYNTHESIS_AVAILABLE = True
except ImportError:
    ATLAS_SYNTHESIS_AVAILABLE = False
    print("‚ö†Ô∏è ATLAS-specific synthesis not available")

# PRISM System - Use ATLAS-specific version
try:
    from src.training_systems.atlas_prism import create_atlas_prism_system
    ATLAS_PRISM_AVAILABLE = True
except ImportError:
    ATLAS_PRISM_AVAILABLE = False
    print("‚ö†Ô∏è ATLAS-specific PRISM not available")
    # Fallback to generic version
    try:
        from src.program_synthesis.prism_system import PRISMSynthesizer, create_prism_system
        PRISM_AVAILABLE = True
    except ImportError:
        PRISM_AVAILABLE = False
        print("‚ö†Ô∏è Generic PRISM not available")

# MEPT and LEAP Systems - Use ATLAS-specific versions
try:
    from src.training_systems.atlas_mept import create_atlas_mept_system, AtlasMEPTLoss
    from src.training_systems.atlas_leap import create_atlas_leap_system, AtlasLEAPTrainer
    ATLAS_MEPT_LEAP_AVAILABLE = True
except ImportError:
    ATLAS_MEPT_LEAP_AVAILABLE = False
    print("‚ö†Ô∏è ATLAS-specific MEPT/LEAP not available")
    # Fallback to generic versions
    try:
        from src.utils.mept_system import ExperienceReplayBuffer, PatternBank, MEPTLoss, create_mept_system
        from src.utils.leap_system import AdaptivePatternGenerator, LEAPTrainer, create_leap_system
        MEPT_LEAP_AVAILABLE = True
    except ImportError:
        MEPT_LEAP_AVAILABLE = False
        print("‚ö†Ô∏è Generic MEPT/LEAP not available")

# LEAP-PRISM Bridge
try:
    from src.utils.leap_prism_bridge import create_leap_prism_bridge, LEAPPatternEnhancer
    LEAP_PRISM_BRIDGE_AVAILABLE = True
except ImportError:
    LEAP_PRISM_BRIDGE_AVAILABLE = False
    print("‚ö†Ô∏è LEAP-PRISM bridge not available")

# Exact Match Injection System
try:
    from stage0_exact_match_boost import ExactMatchBoostDataset, AggressiveLoss, inject_exact_match_training
    EXACT_BOOST_AVAILABLE = True
except ImportError:
    EXACT_BOOST_AVAILABLE = False
    print("‚ö†Ô∏è Exact match boost not available")

# Data systems
from src.data.arc_data_synthesis import ARCDataSynthesizer, ARCDataAugmenter

# Import the ENTIRE original script to build upon it
from train_atlas_specialized import (
    AtlasSpecializedDataset, 
    AtlasSpecializedLoss,
    atlas_exact_match_injection,
    atlas_mept_injection,
    atlas_leap_injection,
    atlas_prism_injection,
    custom_collate_fn,
    train_atlas_specialized as train_atlas_specialized_v1,
    ATLAS_CONFIG as ATLAS_CONFIG_V1,
    STAGE_CONFIG as STAGE_CONFIG_V1
)

# Enhanced ATLAS Configuration V2 - Building on V1 (SLOWED DOWN)
ATLAS_CONFIG = ATLAS_CONFIG_V1.copy()
ATLAS_CONFIG.update({
    # CHRONOS-INSPIRED SUCCESSFUL PARAMETERS
    'batch_size': 256,  # Match CHRONOS successful batch size  
    'learning_rate': 0.002,  # Match CHRONOS successful learning rate
    'gradient_accumulation': 2,  # Effective batch: 512 (match CHRONOS)
    'transform_penalty': 0.3,  # Match CHRONOS transform penalty
    'exact_match_bonus': 2.5,  # Match CHRONOS exact match bonus
    'focal_gamma': 2.0,  # Restore stronger focal loss
    'spatial_weight': 0.2,  # Match CHRONOS temporal weight equivalent
    
    # CHRONOS-style training features
    'use_mixup': True,  # Enable mixup like CHRONOS
    'mixup_alpha': 0.2,
    'gradient_clip': 1.0,  # Match CHRONOS gradient clipping
    'warmup_steps': 1000,  # Match CHRONOS warmup
    'cosine_restarts': True,  # Enable restarts like CHRONOS
    'label_smoothing': 0.1,  # Match CHRONOS smoothing
    
    # CHRONOS-style epoch structure
    'epochs_per_stage': 40,  # Match CHRONOS stage length
    'num_epochs': 320,  # Match CHRONOS total epochs (8 stages x 40)
    'patience': 10,  # Shorter patience like CHRONOS
})

# Enhanced Stage Configuration V2 - CHRONOS-style progressive curriculum
STAGE_CONFIG = STAGE_CONFIG_V1.copy()
# Update with CHRONOS-style settings
for stage in STAGE_CONFIG:
    # CHRONOS-style consistent LR multipliers
    STAGE_CONFIG[stage]['lr_mult'] = 1.0 - (stage * 0.1)  # More gradual decay like CHRONOS
    STAGE_CONFIG[stage]['epochs'] = 40  # Match CHRONOS epoch count per stage

# Add CHRONOS-style exact injection flag for stage 0
STAGE_CONFIG[0]['exact_injection'] = True  # Enable exact match injection like CHRONOS


# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'Using device: {device}')


class AtlasSpecializedLossV2(AtlasSpecializedLoss):
    """Enhanced ATLAS loss with mixup and label smoothing"""
    
    def forward(self, pred_output, target_output, input_grid, model_outputs=None, mixup_lambda=None):
        """Enhanced forward with mixup support"""
        
        # If using mixup, adjust the loss calculation
        if mixup_lambda is not None:
            # Get base losses for both targets
            losses1 = super().forward(pred_output, target_output[0], input_grid, model_outputs)
            losses2 = super().forward(pred_output, target_output[1], input_grid, model_outputs)
            
            # Mix the losses
            mixed_losses = {}
            for key in losses1:
                if torch.is_tensor(losses1[key]):
                    mixed_losses[key] = mixup_lambda * losses1[key] + (1 - mixup_lambda) * losses2[key]
                else:
                    mixed_losses[key] = losses1[key]  # For counts, use first
            
            return mixed_losses
        
        # Apply label smoothing if configured
        if hasattr(self, 'label_smoothing') and self.label_smoothing > 0:
            C = target_output.shape[1]
            smooth_target = target_output * (1 - self.label_smoothing)
            smooth_target += self.label_smoothing / C
            target_output = smooth_target
        
        return super().forward(pred_output, target_output, input_grid, model_outputs)


def mixup_data(x, y, alpha=1.0):
    """Apply mixup augmentation"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)

    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    
    return mixed_x, y_a, y_b, lam


class WarmupCosineSchedule(optim.lr_scheduler._LRScheduler):
    """Cosine learning rate schedule with warmup"""
    
    def __init__(self, optimizer, warmup_steps, training_steps, cycles=1, last_epoch=-1):
        self.warmup_steps = warmup_steps
        self.training_steps = training_steps
        self.cycles = cycles
        super().__init__(optimizer, last_epoch)
    
    def get_lr(self):
        if self.last_epoch < self.warmup_steps:
            # Linear warmup
            return [base_lr * self.last_epoch / self.warmup_steps for base_lr in self.base_lrs]
        else:
            # Cosine annealing
            progress = (self.last_epoch - self.warmup_steps) / (self.training_steps - self.warmup_steps)
            if self.cycles > 1:
                # With restarts
                cycle_progress = progress * self.cycles
                progress = cycle_progress - int(cycle_progress)
            return [base_lr * (0.5 * (1 + np.cos(np.pi * progress))) for base_lr in self.base_lrs]


def train_atlas_specialized_v2():
    """Enhanced ATLAS training building on V1"""
    print("üåç Starting ATLAS Specialized Training V2")
    print("=" * 60)
    print("üìä Enhancements over V1:")
    print("  ‚Ä¢ Mixup augmentation for better generalization")
    print("  ‚Ä¢ Warmup + cosine annealing with restarts")
    print("  ‚Ä¢ Label smoothing")
    print("  ‚Ä¢ Gradient clipping")
    print("  ‚Ä¢ Better batch size and learning rate")
    print("=" * 60)
    
    # Initialize model with maximum grid size from final stage
    max_grid_size = STAGE_CONFIG[7]['max_grid_size']  # 30x30
    model = EnhancedAtlasNet(
        max_grid_size=max_grid_size
    ).to(device)
    
    print(f"üìä ATLAS Model: {sum(p.numel() for p in model.parameters()):,} parameters")
    
    # Initialize all AutomataNexus training systems (reuse V1 logic)
    systems = {}
    
    # MEPT System - Use ATLAS-specific if available
    if USE_MEPT:
        if ATLAS_MEPT_LEAP_AVAILABLE:
            mept_components = create_atlas_mept_system(
                model=model,
                device=device
            )
            systems['spatial_memory'] = mept_components['spatial_memory']
            systems['pattern_bank'] = mept_components['pattern_bank']
            systems['loss_fn'] = mept_components['loss_function']
            print("‚úÖ ATLAS-specific MEPT system initialized")
        else:
            mept_components = create_mept_system(
                capacity=40000,
                pattern_bank_size=8000,
                transformation_penalty=ATLAS_CONFIG['transform_penalty'],
                exact_match_bonus=ATLAS_CONFIG['exact_match_bonus']
            )
            systems['spatial_memory'] = mept_components['replay_buffer'] 
            systems['pattern_bank'] = mept_components['pattern_bank']
            systems['loss_fn'] = mept_components.get('loss_fn')
            print("‚úÖ Generic MEPT system initialized")
    
    # LEAP System - Use ATLAS-specific if available
    if USE_LEAP:
        if ATLAS_MEPT_LEAP_AVAILABLE:
            leap_components = create_atlas_leap_system(model=model, device=device)
            systems['leap_trainer'] = leap_components['trainer']
            systems['pattern_generator'] = leap_components.get('pattern_generator')
            systems['weak_detector'] = leap_components.get('detector')
            print("‚úÖ ATLAS-specific LEAP system initialized")
        else:
            leap_components = create_leap_system(device)
            systems['leap_trainer'] = leap_components['trainer']
            systems['pattern_generator'] = leap_components.get('pattern_generator')
            systems['weak_detector'] = leap_components.get('detector')
            print("‚úÖ Generic LEAP system initialized")
    
    # PRISM System - Use ATLAS-specific if available
    if USE_PRISM:
        if ATLAS_PRISM_AVAILABLE:
            prism_components = create_atlas_prism_system(model=model, device=device)
            systems['prism_synthesizer'] = prism_components['synthesizer']
            systems['prism_library'] = prism_components['program_bank']
            print("‚úÖ ATLAS-specific PRISM system initialized")
        else:
            prism_components = create_prism_system()
            systems['prism_synthesizer'] = prism_components['synthesizer']
            # Generic PRISM doesn't have program_library
            print("‚úÖ Generic PRISM system initialized")
    
    # Initialize V2 enhanced loss
    loss_fn = AtlasSpecializedLossV2().to(device)
    loss_fn.label_smoothing = ATLAS_CONFIG.get('label_smoothing', 0.1)
    
    # Optimizer - AdamW for better regularization
    optimizer = optim.AdamW(
        model.parameters(),
        lr=ATLAS_CONFIG['learning_rate'],
        betas=(0.9, 0.999),
        weight_decay=1e-4
    )
    
    # Calculate total training steps
    total_epochs = ATLAS_CONFIG['num_epochs']
    steps_per_epoch = 100  # Approximate
    total_steps = total_epochs * steps_per_epoch
    
    # Warmup + Cosine scheduler with restarts
    scheduler = WarmupCosineSchedule(
        optimizer,
        warmup_steps=ATLAS_CONFIG['warmup_steps'],
        training_steps=total_steps,
        cycles=3 if ATLAS_CONFIG.get('cosine_restarts') else 1
    )
    
    scaler = GradScaler('cuda')
    
    # Data directory
    DATA_DIR = '/content/AutomataNexus_Olympus_AGI2/data'
    
    # Training metrics
    best_exact = 0.0
    global_epoch = 0
    global_step = 0
    
    # Check for existing best model
    models_dir = '/content/AutomataNexus_Olympus_AGI2/arc_models_v4'
    best_model_path = f'{models_dir}/atlas_best.pt'
    
    if os.path.exists(best_model_path):
        print(f"üîÑ Loading best model from {best_model_path}")
        try:
            checkpoint = torch.load(best_model_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            loaded_exact = checkpoint.get('best_exact', 0.0)
            # For incremental training: reset threshold to allow improvements
            best_exact = 0.0  # Reset to allow stage validation to save improvements
            print(f"‚úÖ Loaded best model with {loaded_exact:.2f}% exact match")
            print(f"üîÑ Reset threshold to {best_exact:.2f}% for incremental training")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load best model: {e}")
            print("üÜï Starting fresh training")
    else:
        print("üÜï No existing model found - starting fresh training")
    
    # Training history
    history = defaultdict(list)
    
    # 8-Stage Progressive Curriculum Training Loop
    stage_metrics = []
    
    # 4-PHASE INJECTION (skip for pre-trained models)
    if USE_EXACT_BOOST and loaded_exact == 0.0:  # Only run on fresh models
        print("\n" + "=" * 60)
        print("üåç ATLAS 4-PHASE SPATIAL TRANSFORMATION INJECTION SEQUENCE")
        print("=" * 60)
        
        # Phase 1: Exact Match (CUSTOM IMPLEMENTATION FOR V2)
        print("\nüìç PHASE 1: Conservative Spatial Identity Mapping")
        print(f"üîÑ Custom injection for pre-trained model (current: {best_exact:.2f}%)")
        
        # Custom exact match injection for V2
        model.train()
        exact_optimizer = optim.Adam(model.parameters(), lr=0.0005, betas=(0.9, 0.999), weight_decay=1e-6)
        
        # Create exact match training samples
        exact_samples = []
        for i in range(120):
            size = random.choice([3, 4, 5])
            input_grid = np.random.randint(0, 4, (size, size))
            # For exact match, output should be identical to input
            output_grid = input_grid.copy()
            exact_samples.append({'inputs': input_grid, 'outputs': output_grid})
        
        # Run exact match injection training
        exact_success = 0
        for epoch in range(25):
            epoch_correct = 0
            epoch_total = 0
            
            for sample in exact_samples:
                inputs = torch.tensor(sample['inputs']).long().unsqueeze(0).to(device)
                outputs = torch.tensor(sample['outputs']).long().unsqueeze(0).to(device)
                
                inputs = torch.clamp(inputs, 0, 9)
                outputs = torch.clamp(outputs, 0, 9)
                
                input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                
                with autocast('cuda'):
                    model_outputs = model(input_grids, output_grids, mode='train')
                    pred_output = model_outputs['predicted_output']
                    losses = loss_fn(pred_output, output_grids, input_grids, model_outputs)
                
                loss = losses['total'] * 0.15  # Gentle loss for exact match
                
                exact_optimizer.zero_grad()
                scaler.scale(loss).backward()
                scaler.step(exact_optimizer)
                scaler.update()
                
                # Check accuracy
                pred_indices = pred_output.argmax(dim=1)
                target_indices = output_grids.argmax(dim=1)
                correct = (pred_indices == target_indices).all().item()
                epoch_correct += correct
                epoch_total += 1
            
            epoch_acc = (epoch_correct / epoch_total) * 100
            if epoch % 5 == 0:
                print(f"   Exact Match Epoch {epoch+1}: {epoch_acc:.1f}% accuracy")
            
            if epoch_acc > 70.0:
                exact_success = epoch_acc
                break
        
        if exact_success > 0:
            print(f"‚úÖ Exact Match injection successful: {exact_success:.1f}% accuracy")
        else:
            print("‚ö†Ô∏è Exact Match injection had low success, continuing anyway")
        
        # Phase 2: MEPT (CUSTOM IMPLEMENTATION FOR V2)
        if USE_MEPT and 'spatial_memory' in systems:
            print("\nüìç PHASE 2: Spatial Memory Enhancement (MEPT)")
            print("üîÑ Running custom MEPT injection for pre-trained model...")
            
            # Custom MEPT injection for V2
            model.train()
            mept_optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-6)
            
            # Create small MEPT training dataset
            mept_samples = []
            for i in range(100):
                size = random.choice([4, 5, 6])
                input_grid = np.random.randint(0, 5, (size, size))
                # Simple transformation patterns for MEPT
                transform = random.choice(['identity', 'rotate', 'flip'])
                if transform == 'identity':
                    output_grid = input_grid.copy()
                elif transform == 'rotate':
                    output_grid = np.rot90(input_grid, k=1).copy()
                else:  # flip
                    output_grid = np.flip(input_grid, axis=0).copy()
                mept_samples.append({'inputs': input_grid, 'outputs': output_grid})
            
            # Run MEPT injection training
            mept_success = 0
            for epoch in range(20):  # Short MEPT training
                epoch_correct = 0
                epoch_total = 0
                
                for sample in mept_samples:
                    inputs = torch.tensor(sample['inputs']).long().unsqueeze(0).to(device)
                    outputs = torch.tensor(sample['outputs']).long().unsqueeze(0).to(device)
                    
                    inputs = torch.clamp(inputs, 0, 9)
                    outputs = torch.clamp(outputs, 0, 9)
                    
                    input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                    output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                    
                    with autocast('cuda'):
                        model_outputs = model(input_grids, output_grids, mode='train')
                        pred_output = model_outputs['predicted_output']
                        losses = loss_fn(pred_output, output_grids, input_grids, model_outputs)
                    
                    loss = losses['total'] * 0.1  # Very gentle loss for MEPT
                    
                    mept_optimizer.zero_grad()
                    scaler.scale(loss).backward()
                    scaler.step(mept_optimizer)
                    scaler.update()
                    
                    # Check accuracy
                    pred_indices = pred_output.argmax(dim=1)
                    target_indices = output_grids.argmax(dim=1)
                    correct = (pred_indices == target_indices).all().item()
                    epoch_correct += correct
                    epoch_total += 1
                
                epoch_acc = (epoch_correct / epoch_total) * 100
                if epoch % 5 == 0:
                    print(f"   MEPT Epoch {epoch+1}: {epoch_acc:.1f}% accuracy")
                
                if epoch_acc > 50.0:
                    mept_success = epoch_acc
                    break
            
            if mept_success > 0:
                print(f"‚úÖ MEPT injection successful: {mept_success:.1f}% accuracy")
            else:
                print("‚ö†Ô∏è MEPT injection had low success, continuing anyway")
        
        # Phase 3: LEAP (CUSTOM IMPLEMENTATION FOR V2)  
        if USE_LEAP and 'leap_trainer' in systems:
            print("\nüìç PHASE 3: Conservative Adaptive Spatial Learning (LEAP)")
            print("üîÑ Running custom LEAP injection for pre-trained model...")
            
            # Custom LEAP injection for V2
            model.train()
            leap_optimizer = optim.Adam(model.parameters(), lr=0.0002, weight_decay=1e-6)
            
            # Create LEAP pattern samples
            leap_samples = []
            for i in range(80):
                size = random.choice([5, 6, 7])
                input_grid = np.random.randint(0, 4, (size, size))
                # LEAP-style spatial patterns
                pattern = random.choice(['shift', 'mirror', 'pattern'])
                if pattern == 'shift':
                    output_grid = np.roll(input_grid, shift=1, axis=random.randint(0, 1))
                elif pattern == 'mirror':
                    output_grid = np.flip(input_grid, axis=random.randint(0, 1)).copy()
                else:  # pattern completion
                    output_grid = input_grid.copy()
                    output_grid[0, 0] = (input_grid[0, 0] + 1) % 4
                leap_samples.append({'inputs': input_grid, 'outputs': output_grid})
            
            # Run LEAP injection training
            leap_success = 0
            for epoch in range(15):
                epoch_correct = 0
                epoch_total = 0
                
                for sample in leap_samples:
                    inputs = torch.tensor(sample['inputs']).long().unsqueeze(0).to(device)
                    outputs = torch.tensor(sample['outputs']).long().unsqueeze(0).to(device)
                    
                    inputs = torch.clamp(inputs, 0, 9)
                    outputs = torch.clamp(outputs, 0, 9)
                    
                    input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                    output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                    
                    with autocast('cuda'):
                        model_outputs = model(input_grids, output_grids, mode='train')
                        pred_output = model_outputs['predicted_output']
                        losses = loss_fn(pred_output, output_grids, input_grids, model_outputs)
                    
                    loss = losses['total'] * 0.08  # Very gentle loss for LEAP
                    
                    leap_optimizer.zero_grad()
                    scaler.scale(loss).backward()
                    scaler.step(leap_optimizer)
                    scaler.update()
                    
                    # Check accuracy
                    pred_indices = pred_output.argmax(dim=1)
                    target_indices = output_grids.argmax(dim=1)
                    correct = (pred_indices == target_indices).all().item()
                    epoch_correct += correct
                    epoch_total += 1
                
                epoch_acc = (epoch_correct / epoch_total) * 100
                if epoch % 5 == 0:
                    print(f"   LEAP Epoch {epoch+1}: {epoch_acc:.1f}% accuracy")
                
                if epoch_acc > 40.0:
                    leap_success = epoch_acc
                    break
            
            if leap_success > 0:
                print(f"‚úÖ LEAP injection successful: {leap_success:.1f}% accuracy")
            else:
                print("‚ö†Ô∏è LEAP injection had low success, continuing anyway")
        
        # Phase 4: PRISM (CUSTOM IMPLEMENTATION FOR V2)
        if USE_PRISM and 'prism_synthesizer' in systems:
            print("\nüìç PHASE 4: Conservative Spatial Program Synthesis (PRISM)")
            print("üîÑ Running custom PRISM injection for pre-trained model...")
            
            # Custom PRISM injection for V2
            model.train()
            prism_optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-6)
            
            # Create PRISM program synthesis samples
            prism_samples = []
            for i in range(60):
                size = random.choice([4, 5, 6])
                input_grid = np.random.randint(0, 3, (size, size))
                # PRISM-style program synthesis patterns
                program = random.choice(['copy', 'invert', 'add_one'])
                if program == 'copy':
                    output_grid = input_grid.copy()
                elif program == 'invert':
                    output_grid = (2 - input_grid).clip(0, 2)
                else:  # add_one
                    output_grid = (input_grid + 1) % 3
                prism_samples.append({'inputs': input_grid, 'outputs': output_grid})
            
            # Run PRISM injection training
            prism_success = 0
            for epoch in range(12):
                epoch_correct = 0
                epoch_total = 0
                
                for sample in prism_samples:
                    inputs = torch.tensor(sample['inputs']).long().unsqueeze(0).to(device)
                    outputs = torch.tensor(sample['outputs']).long().unsqueeze(0).to(device)
                    
                    inputs = torch.clamp(inputs, 0, 9)
                    outputs = torch.clamp(outputs, 0, 9)
                    
                    input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                    output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                    
                    with autocast('cuda'):
                        model_outputs = model(input_grids, output_grids, mode='train')
                        pred_output = model_outputs['predicted_output']
                        losses = loss_fn(pred_output, output_grids, input_grids, model_outputs)
                    
                    loss = losses['total'] * 0.05  # Very gentle loss for PRISM
                    
                    prism_optimizer.zero_grad()
                    scaler.scale(loss).backward()
                    scaler.step(prism_optimizer)
                    scaler.update()
                    
                    # Check accuracy
                    pred_indices = pred_output.argmax(dim=1)
                    target_indices = output_grids.argmax(dim=1)
                    correct = (pred_indices == target_indices).all().item()
                    epoch_correct += correct
                    epoch_total += 1
                
                epoch_acc = (epoch_correct / epoch_total) * 100
                if epoch % 4 == 0:
                    print(f"   PRISM Epoch {epoch+1}: {epoch_acc:.1f}% accuracy")
                
                if epoch_acc > 35.0:
                    prism_success = epoch_acc
                    break
            
            if prism_success > 0:
                print(f"‚úÖ PRISM injection successful: {prism_success:.1f}% accuracy")
            else:
                print("‚ö†Ô∏è PRISM injection had low success, continuing anyway")
        
        print("\n‚úÖ 4-PHASE INJECTION COMPLETE")
        print("=" * 60)
    else:
        if USE_EXACT_BOOST:
            print("\n" + "=" * 60)
            print("‚è≠Ô∏è SKIPPING INJECTION PHASES FOR PRE-TRAINED MODEL")
            print(f"   Pre-trained model accuracy: {loaded_exact:.2f}%")
            print("   Proceeding directly to main curriculum training")
            print("=" * 60)
    
    # Main curriculum training
    for stage in range(ATLAS_CONFIG['curriculum_stages']):
        stage_config = STAGE_CONFIG[stage]
        grid_size = stage_config['max_grid_size']
        
        print(f"\nüåç ATLAS Stage {stage}: {grid_size}x{grid_size} Spatial Transformation")
        print(f"   üìè Grid Size: {grid_size}x{grid_size} | Synthesis: {int(stage_config['synthesis_ratio']*100)}%")
        print("=" * 60)
        
        # CHRONOS-style exact match injection for stage 0
        if stage == 0 and stage_config.get('exact_injection', False):
            print("\nüéØ CHRONOS-STYLE EXACT MATCH INJECTION TRAINING")
            print("=" * 50)
            print("Enhanced with:")
            print("  ‚Ä¢ Focal loss (gamma=2)")
            print("  ‚Ä¢ Progressive exact match bonus (1x-3x)")
            print("  ‚Ä¢ Data augmentation")
            print("  ‚Ä¢ Warmup + cosine annealing")
            print("  ‚Ä¢ 300K total samples")
            print("  ‚Ä¢ TARGET: 93.0% exact match accuracy")
            
            # Create exact match dataset
            exact_match_samples = []
            for i in range(300000):  # Massive dataset like CHRONOS
                size = random.choice([3, 4, 5, 6])
                input_grid = np.random.randint(0, 4, (size, size))
                # 90% exact match tasks for intensive training
                if random.random() < 0.9:
                    output_grid = input_grid.copy()  # Identity
                else:
                    output_grid = (input_grid + 1) % 4  # Simple transform
                exact_match_samples.append({'inputs': input_grid, 'outputs': output_grid})
            
            # Create exact match dataset and loader
            class ExactMatchDataset(Dataset):
                def __init__(self, samples):
                    self.samples = samples
                def __len__(self):
                    return len(self.samples)
                def __getitem__(self, idx):
                    return self.samples[idx]
            
            exact_dataset = ExactMatchDataset(exact_match_samples)
            exact_loader = DataLoader(
                exact_dataset,
                batch_size=ATLAS_CONFIG['batch_size'],
                shuffle=True,
                num_workers=0,
                pin_memory=False,
                collate_fn=lambda batch: custom_collate_fn(batch, 0),
                drop_last=True
            )
            
            # Exact match training
            exact_optimizer = optim.AdamW(model.parameters(), lr=0.00005, weight_decay=1e-6)
            exact_scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(exact_optimizer, T_0=1000)
            
            model.train()
            for epoch in range(1):  # Single intensive epoch
                exact_correct = 0
                exact_total = 0
                
                pbar = tqdm(exact_loader, desc=f"Exact Match Training")
                for batch_idx, batch in enumerate(pbar):
                    inputs = batch['inputs'].to(device)
                    outputs = batch['outputs'].to(device)
                    
                    inputs = torch.clamp(inputs, 0, 9)
                    outputs = torch.clamp(outputs, 0, 9)
                    
                    input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                    output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                    
                    with autocast('cuda'):
                        model_outputs = model(input_grids, output_grids, mode='train')
                        pred_output = model_outputs['predicted_output']
                        losses = loss_fn(pred_output, output_grids, input_grids, model_outputs)
                    
                    loss = losses['total'] * 0.15  # Gentle loss like CHRONOS
                    
                    exact_optimizer.zero_grad()
                    scaler.scale(loss).backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
                    scaler.step(exact_optimizer)
                    scaler.update()
                    exact_scheduler.step()
                    
                    # Track accuracy
                    pred_indices = pred_output.argmax(dim=1)
                    target_indices = output_grids.argmax(dim=1)
                    correct = (pred_indices == target_indices).all(dim=[1,2]).sum().item()
                    exact_correct += correct
                    exact_total += inputs.size(0)
                    
                    if batch_idx % 100 == 0:
                        current_acc = (exact_correct / exact_total) * 100 if exact_total > 0 else 0
                        pbar.set_postfix({
                            'Exact': f'{current_acc:.1f}%',
                            'Loss': f'{loss.item():.3f}',
                            'LR': f'{exact_scheduler.get_last_lr()[0]:.5f}'
                        })
                        
                        # Early stopping if target reached
                        if current_acc > 93.0:
                            print(f"\n\033[96müèÜ TARGET REACHED: {current_acc:.1f}% exact match!\033[0m")
                            break
            
            final_acc = (exact_correct / exact_total) * 100
            print(f"\033[96m‚è∞ Exact match injection completed - Final: {final_acc:.1f}%\033[0m")
            print("=" * 50)
            
            del exact_loader, exact_dataset, exact_match_samples
            gc.collect()
            torch.cuda.empty_cache()
        
        # Create dataset using V1 approach but with smaller size for testing
        print(f"üîß Generating ATLAS-specific DSL spatial patterns for stage {stage}...")
        
        # Initialize DSL trainer
        dsl_trainer = ATLASDSLTraining(model, device)
        print(f"‚úÖ ATLAS DSL spatial pattern trainer initialized")
        
        # CHRONOS-STYLE MASSIVE DATASET GENERATION
        dataset_samples = []
        
        # 1. Load ARC JSON files (base ~800 samples)
        arc_files = ['arc-agi_training_challenges.json', 'arc-agi_evaluation_challenges.json']
        original_arc_samples = []
        
        for filename in arc_files:
            filepath = os.path.join(DATA_DIR, filename)
            if os.path.exists(filepath):
                with open(filepath, 'r') as f:
                    tasks = json.load(f)
                    for task_id, task_data in tasks.items():
                        for example in task_data['train']:
                            input_grid = np.array(example['input'])
                            output_grid = np.array(example['output'])
                            if input_grid.shape[0] <= grid_size and input_grid.shape[1] <= grid_size:
                                original_arc_samples.append({'inputs': input_grid, 'outputs': output_grid})
        
        dataset_samples.extend(original_arc_samples)
        print(f"‚úÖ Loaded {len(original_arc_samples)} original ARC samples")
        
        # 2. CHRONOS-style 10x Data Augmentation
        for sample in original_arc_samples:
            for aug_i in range(10):  # 10x augmentation like CHRONOS
                input_grid = sample['inputs'].copy()
                output_grid = sample['outputs'].copy()
                
                # Random augmentations
                if random.random() < 0.5:  # Rotation
                    k = random.randint(1, 3)
                    input_grid = np.rot90(input_grid, k=k)
                    output_grid = np.rot90(output_grid, k=k)
                if random.random() < 0.5:  # Flip
                    axis = random.randint(0, 1)
                    input_grid = np.flip(input_grid, axis=axis)
                    output_grid = np.flip(output_grid, axis=axis)
                    
                dataset_samples.append({'inputs': input_grid.copy(), 'outputs': output_grid.copy()})
        
        print(f"‚úÖ Added {len(original_arc_samples) * 10} augmented samples")
        
        # 3. Stage 0 Identity Tasks (CHRONOS-style exact match emphasis)  
        if stage == 0:
            identity_samples = []
            for i in range(2000):  # Large number like CHRONOS
                size = random.choice([3, 4, 5, 6])
                input_grid = np.random.randint(0, 4, (size, size))
                # 60% identity tasks for exact match learning
                if random.random() < 0.6:
                    output_grid = input_grid.copy()  # Identity task
                else:
                    # Simple transformations
                    output_grid = (input_grid + 1) % 4  # Color shift
                identity_samples.append({'inputs': input_grid, 'outputs': output_grid})
            
            dataset_samples.extend(identity_samples)
            print(f"‚úÖ Added {len(identity_samples)} identity/simple samples for stage 0")
        
        # 4. DSL Spatial Pattern Samples (match CHRONOS DSL count)
        num_dsl_samples = 500 + stage * 100  # Like CHRONOS: 500-1200 per stage
        for i in range(num_dsl_samples):
            size = min(random.choice([4, 5, 6, 7]), grid_size)
            input_grid = np.random.randint(0, 6, (size, size))
            
            # ATLAS spatial transformations (like CHRONOS temporal patterns)
            transform = random.choice(['rotate', 'flip', 'transpose', 'shift', 'reflect', 'scale'])
            if transform == 'rotate':
                output_grid = np.rot90(input_grid, k=random.randint(1, 3)).copy()
            elif transform == 'flip':
                output_grid = np.flip(input_grid, axis=random.randint(0, 1)).copy()
            elif transform == 'transpose':
                output_grid = input_grid.T.copy()
            elif transform == 'shift':
                output_grid = np.roll(input_grid, shift=random.randint(1, 2), axis=random.randint(0, 1))
            elif transform == 'reflect':
                # Diagonal reflection
                output_grid = input_grid[::-1, ::-1].copy()
            else:  # scale (duplicate pattern)
                output_grid = np.repeat(np.repeat(input_grid, 2, axis=0)[:size], 2, axis=1)[:, :size]
            
            dataset_samples.append({'inputs': input_grid, 'outputs': output_grid})
        
        print(f"‚úÖ Added {num_dsl_samples} DSL spatial pattern samples")
        
        # 5. Program Synthesis Samples (match CHRONOS)
        num_ps_samples = 200 if stage == 0 else 300
        for i in range(num_ps_samples):
            size = min(random.choice([3, 4, 5]), grid_size)
            input_grid = np.random.randint(0, 3, (size, size))
            
            # Programmatic transformations
            program = random.choice(['copy', 'invert', 'add_one', 'multiply'])
            if program == 'copy':
                output_grid = input_grid.copy()
            elif program == 'invert':
                output_grid = (2 - input_grid).clip(0, 2)
            elif program == 'add_one':
                output_grid = (input_grid + 1) % 3
            else:  # multiply
                output_grid = (input_grid * 2) % 3
                
            dataset_samples.append({'inputs': input_grid, 'outputs': output_grid})
        
        print(f"‚úÖ Added {num_ps_samples} program synthesis samples")
        
        # 6. Synthetic ARC Samples (CHRONOS-style massive synthesis)
        synthesis_ratio = 0.5  # 50% more synthetic data
        num_synthetic = int(len(dataset_samples) * synthesis_ratio)
        
        for i in range(num_synthetic):
            size = min(random.choice([4, 5, 6, 7]), grid_size)
            input_grid = np.random.randint(0, min(8, 4 + stage), (size, size))
            
            # Complex spatial patterns for synthesis
            pattern_type = random.choice(['geometric', 'fill', 'pattern', 'transformation'])
            if pattern_type == 'geometric':
                # Create geometric shapes
                output_grid = np.zeros_like(input_grid)
                for y in range(size):
                    for x in range(size):
                        if (x - size//2)**2 + (y - size//2)**2 <= (size//3)**2:
                            output_grid[y, x] = (input_grid[y, x] + 1) % 8
                        else:
                            output_grid[y, x] = input_grid[y, x]
            elif pattern_type == 'fill':
                # Color region filling
                dominant_color = np.argmax(np.bincount(input_grid.flatten()))
                output_grid = np.full_like(input_grid, dominant_color)
            elif pattern_type == 'pattern':
                # Create repeating patterns
                pattern = input_grid[:2, :2] if size >= 2 else input_grid
                output_grid = np.tile(pattern, ((size + pattern.shape[0] - 1) // pattern.shape[0],
                                              (size + pattern.shape[1] - 1) // pattern.shape[1]))[:size, :size]
            else:  # transformation
                # Complex transformation
                output_grid = np.rot90(np.flip(input_grid, axis=0), k=1)
                
            dataset_samples.append({'inputs': input_grid, 'outputs': output_grid})
        
        print(f"‚úÖ Added {num_synthetic} synthetic ARC samples")
        
        print(f"üéØ TOTAL DATASET SIZE: {len(dataset_samples)} samples (CHRONOS-style massive generation)")
        
        
        # Create dataset
        class SimpleARCDataset(Dataset):
            def __init__(self, samples):
                self.samples = samples
            def __len__(self):
                return len(self.samples)
            def __getitem__(self, idx):
                return self.samples[idx]
        
        dataset = SimpleARCDataset(dataset_samples)
        train_size = int(0.9 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        
        print(f"üìö Stage {stage} ({grid_size}x{grid_size}) - Train: {train_size}, Val: {val_size}")
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=ATLAS_CONFIG['batch_size'],
            shuffle=True,
            num_workers=0,
            pin_memory=False,
            collate_fn=lambda batch: custom_collate_fn(batch, stage),
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=ATLAS_CONFIG['batch_size'],
            shuffle=False,
            num_workers=0,
            pin_memory=False,
            collate_fn=lambda batch: custom_collate_fn(batch, stage),
            drop_last=False
        )
        
        # Adjust learning rate for stage - CHRONOS-style consistent LR
        stage_lr = ATLAS_CONFIG['learning_rate'] * stage_config['lr_mult']
        # Use CHRONOS-style consistent learning rates (no pre-trained boosting)
        for param_group in optimizer.param_groups:
            param_group['lr'] = stage_lr
        print(f"üìà Stage {stage} learning rate: {stage_lr:.6f} (CHRONOS-style)")
        
        # Stage training loop
        print(f"\nüîÑ Training Stage {stage} for {ATLAS_CONFIG['epochs_per_stage']} epochs...")
        
        for epoch in range(ATLAS_CONFIG['epochs_per_stage']):
            global_epoch += 1
            
            # Training phase
            model.train()
            train_metrics = defaultdict(float)
            
            pbar = tqdm(train_loader, desc=f"Stage {stage}, Epoch {epoch+1}")
            
            for batch_idx, batch in enumerate(pbar):
                global_step += 1
                
                inputs = batch['inputs'].to(device, non_blocking=True)
                outputs = batch['outputs'].to(device, non_blocking=True)
                
                # Clamp values
                inputs = torch.clamp(inputs, 0, 9)
                outputs = torch.clamp(outputs, 0, 9)
                
                # Convert to one-hot
                input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                
                # Apply mixup if enabled
                if ATLAS_CONFIG.get('use_mixup') and random.random() < 0.5:
                    mixed_input, target_a, target_b, lam = mixup_data(
                        input_grids, output_grids, alpha=ATLAS_CONFIG.get('mixup_alpha', 0.2)
                    )
                    
                    with autocast('cuda'):
                        model_outputs = model(mixed_input, target_a, mode='train')
                        pred_output = model_outputs['predicted_output']
                        losses = loss_fn(pred_output, (target_a, target_b), mixed_input, 
                                       model_outputs, mixup_lambda=lam)
                else:
                    with autocast('cuda'):
                        model_outputs = model(input_grids, output_grids, mode='train')
                        pred_output = model_outputs['predicted_output']
                        losses = loss_fn(pred_output, output_grids, input_grids, model_outputs)
                
                # Add exact match bonus reward
                loss = losses['total'] / ATLAS_CONFIG['gradient_accumulation']
                if losses.get('exact_count', 0) > 0:
                    # Bonus reward for exact matches
                    exact_bonus = losses['exact_count'] * 0.2 
                    loss = loss - exact_bonus  # Negative loss = reward
                
                # Backward pass
                scaler.scale(loss).backward()
                
                # Gradient accumulation
                if (batch_idx + 1) % ATLAS_CONFIG['gradient_accumulation'] == 0:
                    # Gradient clipping
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), ATLAS_CONFIG['gradient_clip'])
                    
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    
                    # Update scheduler
                    scheduler.step()
                
                # Update metrics
                train_metrics['loss'] += losses['total'].item()
                train_metrics['exact'] += losses['exact_count'].item()
                train_metrics['samples'] += inputs.size(0)
                
                pbar.set_postfix({
                    'loss': f"{losses['total'].item():.3f}",
                    'exact': f"{losses['exact_count'].item():.0f}",
                    'lr': f"{scheduler.get_last_lr()[0]:.6f}"
                })
            
            # Validation phase - check more frequently to catch improvements
            if epoch % 5 == 0 or epoch == ATLAS_CONFIG['epochs_per_stage'] - 1:
                model.eval()
                val_metrics = defaultdict(float)
                
                with torch.no_grad():
                    for batch in tqdm(val_loader, desc="Validation"):
                        inputs = batch['inputs'].to(device, non_blocking=True)
                        outputs = batch['outputs'].to(device, non_blocking=True)
                        
                        inputs = torch.clamp(inputs, 0, 9)
                        outputs = torch.clamp(outputs, 0, 9)
                        
                        input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                        output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                        
                        with autocast('cuda'):
                            model_outputs = model(input_grids, mode='inference')
                            pred_output = model_outputs['predicted_output']
                            losses = loss_fn(pred_output, output_grids, input_grids, model_outputs)
                        
                        pred_indices = pred_output.argmax(dim=1)
                        target_indices = output_grids.argmax(dim=1)
                        pixel_acc = (pred_indices == target_indices).float().mean()
                        
                        val_metrics['loss'] += losses['total'].item()
                        val_metrics['exact'] += losses['exact_count'].item()
                        val_metrics['pixel_acc'] += pixel_acc.item()
                        val_metrics['samples'] += inputs.size(0)
                
                # Calculate averages
                train_loss = train_metrics['loss'] / len(train_loader)
                train_exact_pct = train_metrics['exact'] / train_metrics['samples'] * 100
                val_loss = val_metrics['loss'] / len(val_loader)
                val_exact_pct = val_metrics['exact'] / val_metrics['samples'] * 100
                val_pixel_acc = val_metrics['pixel_acc'] / len(val_loader) * 100
                
                print(f"\nüåç ATLAS Epoch {epoch+1} (Stage {stage}, {grid_size}x{grid_size}):")
                print(f"   üåç GRID SIZE: {grid_size}x{grid_size} | SPATIAL LEARNING: {'üìà' if val_exact_pct > best_exact else '‚û°Ô∏è'}")
                print(f"   üéØ Train: {train_exact_pct:.2f}% exact, Loss: {train_loss:.3f}")
                print(f"   üéØ Val: {val_exact_pct:.2f}% exact, Loss: {val_loss:.3f}, Pixel: {val_pixel_acc:.1f}%")
                print(f"   üìè Stage Progress: {int((epoch+1)/ATLAS_CONFIG['epochs_per_stage']*100)}% | Total Progress: {int((stage*ATLAS_CONFIG['epochs_per_stage']+epoch+1)/(ATLAS_CONFIG['curriculum_stages']*ATLAS_CONFIG['epochs_per_stage'])*100)}%")
                if val_exact_pct > 10:
                    print(f"\033[96m   üìä STATUS: üèÜ EXCELLENT spatial learning for {grid_size}x{grid_size} grids!\033[0m")
                elif val_exact_pct > 5:
                    print(f"\033[96m   üìä STATUS: üìà GOOD spatial progress on {grid_size}x{grid_size} transformations\033[0m")
                else:
                    print(f"\033[96m   üìä STATUS: üìà GOOD spatial progress on {grid_size}x{grid_size} transformations\033[0m")
                
                # Save history
                history['train_loss'].append(train_loss)
                history['train_exact'].append(train_exact_pct)
                history['val_loss'].append(val_loss)
                history['val_exact'].append(val_exact_pct)
                history['learning_rate'].append(scheduler.get_last_lr()[0])
                
                # Save best model
                if val_exact_pct > best_exact:
                    best_exact = val_exact_pct
                    torch.save({
                        'epoch': global_epoch,
                        'stage': stage,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'best_exact': best_exact,
                        'val_loss': val_loss
                    }, f'/content/AutomataNexus_Olympus_AGI2/arc_models_v4/atlas_best.pt')
                    print(f"\033[96m   üíæ NEW BEST: {best_exact:.2f}% spatial exact match saved!\033[0m")
                
                # Early stopping if training improving but validation stuck
                if train_exact_pct > 2.0 and val_exact_pct == 0.0 and epoch > 30:
                    print(f"   ‚ö° Early stop: Training {train_exact_pct:.2f}% but validation stuck at 0.00%")
                    break
        
        # Stage complete
        stage_exact = train_exact_pct
        stage_metrics.append({
            'stage': stage,
            'grid_size': grid_size,
            'final_exact': stage_exact
        })
        
        print(f"\n‚úÖ Stage {stage} complete! Final exact: {stage_exact:.2f}%")
        
        # Clear memory
        del train_loader, val_loader, dataset
        gc.collect()
        torch.cuda.empty_cache()
    
    # Training complete
    print("\n" + "=" * 60)
    print("üéâ ATLAS V2 8-Stage Training Complete!")
    print(f"   üèÜ Best exact match: {best_exact:.2f}%")
    print(f"   üìè Stages completed: 8 (6x6 ‚Üí 30x30 grids)")
    print(f"   üìä Total epochs: {global_epoch}")
    
    print(f"\nüìè Stage-by-stage Spatial Learning Progression:")
    for metrics in stage_metrics:
        print(f"   Stage {metrics['stage']} ({metrics['grid_size']}x{metrics['grid_size']}): "
              f"{metrics['final_exact']:.2f}% exact match")
    
    return model, history


# Training components flags (from V1)
USE_MEPT = True and (ATLAS_MEPT_LEAP_AVAILABLE or MEPT_LEAP_AVAILABLE)
USE_LEAP = True and (ATLAS_MEPT_LEAP_AVAILABLE or MEPT_LEAP_AVAILABLE)
USE_PRISM = True and (ATLAS_PRISM_AVAILABLE or PRISM_AVAILABLE)
USE_EXACT_BOOST = True and EXACT_BOOST_AVAILABLE
USE_LEAP_PRISM_BRIDGE = True and LEAP_PRISM_BRIDGE_AVAILABLE



if __name__ == "__main__":
    print("=" * 80)
    print("ATLAS Specialized Training V2 - Building on V1")
    print("Spatial Transformation Expert with Targeted Enhancements")
    print("=" * 80)
    
    train_atlas_specialized_v2()