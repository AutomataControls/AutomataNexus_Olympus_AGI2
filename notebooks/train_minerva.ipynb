{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# MINERVA Model Training\n\n**Strategic Pattern Analysis Specialist**\n\nThis notebook trains the MINERVA model, which specializes in:\n- Pattern recognition and analysis\n- Strategic reasoning\n- Global pattern understanding\n- Grid attention mechanisms\n- Object relationship analysis",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup Environment",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository and install dependencies\n!git clone https://github.com/AutomataControls/AutomataNexus_Olympus_AGI2.git\n!cd AutomataNexus_Olympus_AGI2 && pip install -r requirements.txt -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# üîß Initialize MINERVA Specialized Training Controller (RUN THIS FIRST!)\nimport json\nimport os\nimport datetime\nfrom typing import Dict, List\n\nclass MinervaSpecializedTrainer:\n    def __init__(self, base_dir='/content/AutomataNexus_Olympus_AGI2'):\n        self.base_dir = base_dir\n        self.iteration_log_file = f\"{base_dir}/arc_models_v4/minerva_specialized_iterations.json\"\n        self.current_params = self.load_default_params()\n        self.iterations = self.load_iteration_history()\n    \n    def load_default_params(self) -> Dict:\n        return {\n            'batch_size': 256,  # MINERVA-optimized smaller batch for attention\n            'learning_rate': 0.008,  # Higher for grid attention learning\n            'hidden_dim': 256,\n            'pattern_memory_size': 200,\n            'attention_heads': 8,\n            'epochs_per_stage': 100,\n            'transform_penalty': 0.5,\n            'exact_match_bonus': 7.0,  # Higher for MINERVA precision\n            'relational_weight': 0.4,  # MINERVA-specific loss\n            'pattern_memory_weight': 0.3,  # MINERVA-specific loss\n            'gradient_accumulation': 2,  # Effective batch: 512\n            'use_mept': True,\n            'use_leap': True,\n            'use_prism': True,\n            'use_exact_boost': True,\n            'use_minerva_dsl': True,  # NEW: MINERVA-specific DSL\n            'curriculum_stages': 3,\n            'max_grid_size': 30\n        }\n    \n    def load_iteration_history(self) -> List[Dict]:\n        if os.path.exists(self.iteration_log_file):\n            with open(self.iteration_log_file, 'r') as f:\n                return json.load(f)\n        return []\n    \n    def save_iteration_history(self):\n        os.makedirs(os.path.dirname(self.iteration_log_file), exist_ok=True)\n        with open(self.iteration_log_file, 'w') as f:\n            json.dump(self.iterations, f, indent=2)\n    \n    def log_iteration(self, params: Dict, results: Dict):\n        iteration = {\n            'iteration': len(self.iterations) + 1,\n            'timestamp': datetime.datetime.now().isoformat(),\n            'parameters': params.copy(),\n            'results': results.copy()\n        }\n        self.iterations.append(iteration)\n        self.save_iteration_history()\n    \n    def get_best_iteration(self) -> Dict:\n        if not self.iterations:\n            return None\n        return max(self.iterations, key=lambda x: x['results'].get('best_exact', 0))\n    \n    def suggest_next_params(self) -> Dict:\n        if len(self.iterations) < 2:\n            return self.current_params.copy()\n        \n        best = self.get_best_iteration()\n        latest = self.iterations[-1]\n        \n        suggestions = best['parameters'].copy() if best else self.current_params.copy()\n        \n        if latest and best:\n            latest_exact = latest['results'].get('best_exact', 0)\n            best_exact = best['results'].get('best_exact', 0)\n            \n            if latest_exact < 5.0:  # Very low performance\n                suggestions['learning_rate'] = min(0.01, suggestions['learning_rate'] * 1.5)\n                suggestions['exact_match_bonus'] = min(10.0, suggestions['exact_match_bonus'] + 1.0)\n                suggestions['relational_weight'] = max(0.2, suggestions['relational_weight'] - 0.1)\n            elif latest_exact < best_exact * 0.8:\n                suggestions['learning_rate'] *= 0.8\n                suggestions['pattern_memory_weight'] = min(0.5, suggestions['pattern_memory_weight'] + 0.1)\n            elif latest_exact > best_exact * 1.1:\n                suggestions['learning_rate'] = min(0.01, suggestions['learning_rate'] * 1.2)\n                suggestions['relational_weight'] = min(0.6, suggestions['relational_weight'] + 0.1)\n        \n        return suggestions\n    \n    def display_history(self):\n        if not self.iterations:\n            print(\"No MINERVA specialized iterations found. Starting fresh!\")\n            return\n        \n        print(\"üß† MINERVA Specialized Training History:\")\n        print(\"-\" * 95)\n        for i, iteration in enumerate(self.iterations):\n            exact = iteration['results'].get('best_exact', 0)\n            loss = iteration['results'].get('best_val_loss', float('inf'))\n            lr = iteration['parameters'].get('learning_rate', 0)\n            rel_weight = iteration['parameters'].get('relational_weight', 0)\n            pattern_weight = iteration['parameters'].get('pattern_memory_weight', 0)\n            timestamp = iteration['timestamp'][:16]\n            \n            status = \"üü¢ BEST\" if iteration == self.get_best_iteration() else \"‚ö™\"\n            print(f\"{status} Iter {i+1}: {exact:.2f}% exact | Loss: {loss:.4f} | LR: {lr:.4f} | Rel: {rel_weight:.2f} | Pat: {pattern_weight:.2f} | {timestamp}\")\n        \n        print(\"-\" * 95)\n        best = self.get_best_iteration()\n        if best:\n            print(f\"üèÜ Best: Iteration {best['iteration']} with {best['results']['best_exact']:.2f}% exact match\")\n        \n        print(f\"\\nüß† MINERVA Specialized Features:\")\n        print(f\"   ‚úÖ Grid Attention with {self.current_params['attention_heads']} heads\")\n        print(f\"   ‚úÖ Pattern Memory Bank: {self.current_params['pattern_memory_size']} patterns\")\n        print(f\"   ‚úÖ Relational Reasoning Loss: {self.current_params['relational_weight']}\")\n        print(f\"   ‚úÖ Object Detection & Boundary Focus\")\n        print(f\"   ‚úÖ Strategic Analysis Optimization\")\n        print(f\"   üÜï MINERVA-Specific DSL Integration for Grid Reasoning\")\n\nminerva_specialized_trainer = MinervaSpecializedTrainer()\nsuggested_params = minerva_specialized_trainer.suggest_next_params()\nminerva_specialized_trainer.display_history()\nprint(\"\\n‚úÖ MINERVA specialized trainer initialized! Ready for grid reasoning training.\")\nprint(\"üÜï Now with MINERVA-specific DSL integration for deterministic grid pattern generation!\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Start MINERVA Specialized Training\n\n**MINERVA-Specific Configuration:**\n- **Architecture**: Grid Attention + Relational Networks + Pattern Memory\n- **Parameters**: ~2.1M\n- **Batch Size**: 256 (optimized for attention mechanisms)\n- **Learning Rate**: 0.008 (higher for grid attention learning)\n- **Hidden Dimensions**: 256\n- **Pattern Memory**: 200 learnable patterns\n- **Attention Heads**: 8\n- **Specialized Loss Components**: Relational reasoning + Pattern memory utilization\n- **ALL Novel Training Methods**: MEPT, LEAP, PRISM, Exact Match Injection, Curriculum Learning\n- **üÜï MINERVA-Specific DSL**: Generates deterministic grid patterns for each curriculum stage\n- **üÜï MINERVA-Specific LEAP**: Focuses on grid-based patterns and spatial relationships\n- **üÜï MINERVA-Specific MEPT**: Stores and retrieves grid patterns with spatial awareness\n- **üÜï MINERVA-Specific PRISM**: Synthesizes grid transformation programs",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Pull latest updates before training\n!cd /content/AutomataNexus_Olympus_AGI2 && git pull"
  },
  {
   "cell_type": "markdown",
   "source": "!cd /content/AutomataNexus_Olympus_AGI2 && python scripts/training/train_minerva_specialized.py",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Method 1: Download ZIP archive (try this first)\nfrom google.colab import files\nimport os\nimport zipfile\nfrom datetime import datetime\n\n# Create ZIP archive\nmodels_dir = '/content/AutomataNexus_Olympus_AGI2/arc_models_v4'\n\n# Create directory if it doesn't exist\nos.makedirs(models_dir, exist_ok=True)\n\n# Check if any MINERVA models exist\nminerva_models = [f for f in os.listdir(models_dir) if f.startswith('minerva_') and f.endswith('.pt')]\n\nif minerva_models:\n    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n    zip_filename = f'/content/minerva_models_{timestamp}.zip'\n    \n    print(\"Creating ZIP archive of MINERVA models...\")\n    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for file in minerva_models:\n            file_path = os.path.join(models_dir, file)\n            zipf.write(file_path, file)\n            size_mb = os.path.getsize(file_path) / (1024**2)\n            print(f\"  Added: {file} ({size_mb:.1f} MB)\")\n    \n    zip_size = os.path.getsize(zip_filename) / (1024**2)\n    print(f\"\\nZIP created: {zip_size:.1f} MB\")\n    \n    print(\"Downloading ZIP file...\")\n    try:\n        files.download(zip_filename)\n        print(\"‚úÖ Download successful!\")\n    except Exception as e:\n        print(f\"‚ùå Download failed: {e}\")\n        print(\"Try Method 2 below\")\nelse:\n    print(\"‚ùå No MINERVA models found yet!\")\n    print(\"Please run the training cell first to generate models.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Method 2: Download individual model files\nfrom google.colab import files\nimport os\n\nmodels_dir = '/content/AutomataNexus_Olympus_AGI2/arc_models_v4'\n\n# Create directory if it doesn't exist\nos.makedirs(models_dir, exist_ok=True)\n\n# Download MINERVA models individually\nminerva_files = ['minerva_best.pt', 'minerva_checkpoint.pt']\nfound_models = False\n\nfor model_name in minerva_files:\n    model_path = os.path.join(models_dir, model_name)\n    if os.path.exists(model_path):\n        found_models = True\n        size_mb = os.path.getsize(model_path) / (1024**2)\n        print(f\"Downloading {model_name} ({size_mb:.1f} MB)...\")\n        try:\n            files.download(model_path)\n            print(f\"‚úÖ {model_name} downloaded!\")\n        except Exception as e:\n            print(f\"‚ùå Failed to download {model_name}: {e}\")\n    else:\n        print(f\"‚ö†Ô∏è {model_name} not found (training may not have created it yet)\")\n\nif not found_models:\n    print(\"\\n‚ùå No MINERVA models found yet!\")\n    print(\"Please run the training cell first to generate models.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Method 3: Google Drive backup (most reliable)\nfrom google.colab import drive\nimport shutil\nimport os\nfrom datetime import datetime\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\n# Create backup folder\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nbackup_dir = f'/content/drive/MyDrive/MINERVA_Models_{timestamp}'\nos.makedirs(backup_dir, exist_ok=True)\n\n# Create models directory if it doesn't exist\nmodels_dir = '/content/AutomataNexus_Olympus_AGI2/arc_models_v4'\nos.makedirs(models_dir, exist_ok=True)\n\n# Copy MINERVA models to Google Drive\nminerva_files = ['minerva_best.pt', 'minerva_checkpoint.pt']\nfound_models = False\n\nprint(f\"Checking for MINERVA models in: {models_dir}\")\nfor model_name in minerva_files:\n    src_path = os.path.join(models_dir, model_name)\n    if os.path.exists(src_path):\n        found_models = True\n        dst_path = os.path.join(backup_dir, model_name)\n        shutil.copy2(src_path, dst_path)\n        size_mb = os.path.getsize(src_path) / (1024**2)\n        print(f\"‚úÖ Copied {model_name} ({size_mb:.1f} MB)\")\n    else:\n        print(f\"‚ö†Ô∏è {model_name} not found\")\n\nif found_models:\n    print(f\"\\nüéâ MINERVA models backed up to Google Drive!\")\n    print(f\"üìÇ Location: {backup_dir}\")\n    print(\"You can now download them from your Google Drive.\")\nelse:\n    print(\"\\n‚ùå No MINERVA models found yet!\")\n    print(\"Please run the training cell first to generate models.\")\n    print(f\"Models will be saved to: {models_dir}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU usage and training progress\n",
    "import subprocess\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Show GPU status\n",
    "gpu_info = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n",
    "print(\"GPU Status:\")\n",
    "print(gpu_info.stdout)\n",
    "\n",
    "# Check for MINERVA checkpoints\n",
    "checkpoints = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/minerva_*.pt')\n",
    "if checkpoints:\n",
    "    print(\"\\nMINERVA Checkpoints:\")\n",
    "    for checkpoint in sorted(checkpoints):\n",
    "        size = os.path.getsize(checkpoint) / (1024**2)  # Size in MB\n",
    "        mtime = os.path.getmtime(checkpoint)\n",
    "        print(f\"  {os.path.basename(checkpoint)}: {size:.1f} MB - {time.ctime(mtime)}\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# MINERVA Specialized Training Execution\nimport subprocess\nimport tempfile\nimport os\n\nif 'minerva_specialized_trainer' not in globals():\n    print(\"Error: Run the specialized trainer initialization cell first!\")\n    raise NameError(\"minerva_specialized_trainer not defined\")\n\n# Use suggested parameters from the specialized trainer\nBATCH_SIZE = suggested_params.get('batch_size', 256)\nLEARNING_RATE = suggested_params.get('learning_rate', 0.008)\nHIDDEN_DIM = suggested_params.get('hidden_dim', 256)\nPATTERN_MEMORY_SIZE = suggested_params.get('pattern_memory_size', 200)\nATTENTION_HEADS = suggested_params.get('attention_heads', 8)\nEPOCHS_PER_STAGE = suggested_params.get('epochs_per_stage', 100)\nTRANSFORM_PENALTY = suggested_params.get('transform_penalty', 0.5)\nEXACT_MATCH_BONUS = suggested_params.get('exact_match_bonus', 7.0)\nRELATIONAL_WEIGHT = suggested_params.get('relational_weight', 0.4)\nPATTERN_MEMORY_WEIGHT = suggested_params.get('pattern_memory_weight', 0.3)\nGRADIENT_ACCUMULATION = suggested_params.get('gradient_accumulation', 2)\nMAX_GRID_SIZE = suggested_params.get('max_grid_size', 30)\n\nprint(f\"üß† Starting MINERVA specialized training iteration {len(minerva_specialized_trainer.iterations) + 1}\")\nprint(f\"Grid Attention Config: LR={LEARNING_RATE}, BS={BATCH_SIZE}, Hidden={HIDDEN_DIM}\")\nprint(f\"Pattern Memory: {PATTERN_MEMORY_SIZE} patterns, {ATTENTION_HEADS} attention heads\")\nprint(f\"Loss Components: Relational={RELATIONAL_WEIGHT}, Pattern Memory={PATTERN_MEMORY_WEIGHT}\")\nprint(f\"Transform Penalty={TRANSFORM_PENALTY}, Exact Bonus={EXACT_MATCH_BONUS}\")\n\nmodified_script = f\"\"\"\nimport sys\nsys.path.append('/content/AutomataNexus_Olympus_AGI2')\nsys.path.append('/content/AutomataNexus_Olympus_AGI2/src')\n\nimport scripts.training.train_minerva_specialized as train_module\n\n# Override MINERVA config with current parameters\ntrain_module.MINERVA_CONFIG['batch_size'] = {BATCH_SIZE}\ntrain_module.MINERVA_CONFIG['learning_rate'] = {LEARNING_RATE}\ntrain_module.MINERVA_CONFIG['hidden_dim'] = {HIDDEN_DIM}\ntrain_module.MINERVA_CONFIG['pattern_memory_size'] = {PATTERN_MEMORY_SIZE}\ntrain_module.MINERVA_CONFIG['attention_heads'] = {ATTENTION_HEADS}\ntrain_module.MINERVA_CONFIG['epochs_per_stage'] = {EPOCHS_PER_STAGE}\ntrain_module.MINERVA_CONFIG['transform_penalty'] = {TRANSFORM_PENALTY}\ntrain_module.MINERVA_CONFIG['exact_match_bonus'] = {EXACT_MATCH_BONUS}\ntrain_module.MINERVA_CONFIG['relational_weight'] = {RELATIONAL_WEIGHT}\ntrain_module.MINERVA_CONFIG['pattern_memory_weight'] = {PATTERN_MEMORY_WEIGHT}\ntrain_module.MINERVA_CONFIG['gradient_accumulation'] = {GRADIENT_ACCUMULATION}\ntrain_module.MINERVA_CONFIG['max_grid_size'] = {MAX_GRID_SIZE}\n\nprint(\"üß† MINERVA Config Updated:\")\nfor key, value in train_module.MINERVA_CONFIG.items():\n    print(f\"   {key}: {value}\")\n\n# Start specialized training\ntry:\n    model, best_exact = train_module.train_minerva_specialized()\n    print(f\"\\\\nüéâ MINERVA specialized training completed! Best: {best_exact:.2f}%\")\nexcept Exception as e:\n    print(f\"‚ùå Training failed: {e}\")\n    import traceback\n    traceback.print_exc()\n\"\"\"\n\nwith tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n    f.write(modified_script)\n    script_path = f.name\n\ntry:\n    print(\"üß† MINERVA specialized training timeout: 10 hours\")\n    result = subprocess.run(\n        ['python', script_path],\n        cwd='/content/AutomataNexus_Olympus_AGI2',\n        capture_output=True,\n        text=True,\n        timeout=36000  # 10 hours\n    )\n    \n    print(\"MINERVA specialized training output:\")\n    print(result.stdout)\n    if result.stderr:\n        print(\"Errors:\")\n        print(result.stderr)\n        \nfinally:\n    if os.path.exists(script_path):\n        os.unlink(script_path)",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Log MINERVA Specialized Results and Update History\nimport glob\nimport torch\nimport json\n\ncurrent_params = {\n    'batch_size': BATCH_SIZE,\n    'learning_rate': LEARNING_RATE,\n    'hidden_dim': HIDDEN_DIM,\n    'pattern_memory_size': PATTERN_MEMORY_SIZE,\n    'attention_heads': ATTENTION_HEADS,\n    'epochs_per_stage': EPOCHS_PER_STAGE,\n    'transform_penalty': TRANSFORM_PENALTY,\n    'exact_match_bonus': EXACT_MATCH_BONUS,\n    'relational_weight': RELATIONAL_WEIGHT,\n    'pattern_memory_weight': PATTERN_MEMORY_WEIGHT,\n    'gradient_accumulation': GRADIENT_ACCUMULATION,\n    'max_grid_size': MAX_GRID_SIZE\n}\n\n# Look for model files in the correct directory\nmodel_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/minerva_best.pt')\nresults = {}\n\nif model_files:\n    checkpoint = torch.load(model_files[0], map_location='cpu')\n    results = {\n        'best_exact': checkpoint.get('val_exact', 0),\n        'best_val_loss': checkpoint.get('val_loss', float('inf')),\n        'training_epoch': checkpoint.get('epoch', 0),\n        'stage': checkpoint.get('stage', 0)\n    }\n    \n    print(f\"üß† MINERVA Specialized Training completed!\")\n    print(f\"   Best exact match: {results['best_exact']:.2f}%\")\n    print(f\"   Best validation loss: {results['best_val_loss']:.4f}\")\n    print(f\"   Training epochs: {results['training_epoch']}\")\n    print(f\"   Final stage: {results['stage']}\")\n    \n    minerva_specialized_trainer.log_iteration(current_params, results)\n    \n    print(\"\\n\" + \"=\"*70)\n    minerva_specialized_trainer.display_history()\n    \n    print(\"\\nüîÆ Suggestions for next MINERVA iteration:\")\n    next_params = minerva_specialized_trainer.suggest_next_params()\n    \n    current_exact = results['best_exact']\n    best_iteration = minerva_specialized_trainer.get_best_iteration()\n    best_exact = best_iteration['results']['best_exact'] if best_iteration else 0\n    \n    if current_exact >= best_exact * 0.95:\n        print(\"   üéØ Performance is good! Try:\")\n        if current_exact > best_exact:\n            print(f\"   üèÜ New best! Consider fine-tuning grid attention with LR={next_params['learning_rate']:.4f}\")\n            print(f\"   üß† Increase pattern memory utilization: weight={next_params['pattern_memory_weight']:.2f}\")\n        else:\n            print(f\"   üîß Try optimizing relational reasoning weight: {next_params['relational_weight']:.2f}\")\n            print(f\"   üìä Consider increasing pattern memory size or attention heads\")\n    else:\n        print(\"   ‚ö†Ô∏è Performance below expectations. MINERVA-specific suggestions:\")\n        print(f\"   üîß Adjust grid attention learning rate: {next_params['learning_rate']:.4f}\")\n        print(f\"   üéØ Increase exact match bonus: {next_params['exact_match_bonus']:.1f}\")\n        print(f\"   üß† Optimize relational reasoning: {next_params['relational_weight']:.2f}\")\n        print(f\"   üìã Check pattern memory utilization: {next_params['pattern_memory_weight']:.2f}\")\n        print(\"   ‚úÖ Ensure all specialized features are enabled (MEPT, LEAP, PRISM)\")\n    \nelse:\n    print(\"‚ùå No MINERVA model checkpoint found. Training may have failed.\")\n    print(\"Check the training output above for errors.\")\n    \nprint(\"\\nüìù To continue MINERVA training, modify parameters above and re-run the training cell.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Iterative Training & Hyperparameter Tuning\n\n**Enhanced Training Loop with Checkpoint Resuming**\n\nThis section allows you to:\n- Resume training from checkpoints\n- Adjust hyperparameters between iterations\n- Track performance improvements\n- Get automated suggestions for next iteration",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load and evaluate MINERVA model\nimport torch\nimport json\nimport glob\nimport os\n\nmodel_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/minerva_best.pt')\n\nif model_files:\n    model_file = model_files[0]\n    checkpoint = torch.load(model_file, map_location='cpu')\n    \n    print(\"üß† MINERVA Training Results:\")\n    print(f\"  Best Validation Exact Match: {checkpoint.get('val_exact', 'N/A'):.2f}%\")\n    print(f\"  Training Epoch: {checkpoint.get('epoch', 'N/A')}\")\n    print(f\"  Curriculum Stage: {checkpoint.get('stage', 'N/A')}\")\n    print(f\"  Model Size: {os.path.getsize(model_file) / (1024**2):.1f} MB\")\n    \n    # Display MINERVA-specific config\n    config = checkpoint.get('config', {})\n    if config:\n        print(f\"\\nüß† MINERVA Configuration:\")\n        print(f\"  Grid Attention Heads: {config.get('attention_heads', 'N/A')}\")\n        print(f\"  Pattern Memory Size: {config.get('pattern_memory_size', 'N/A')}\")\n        print(f\"  Hidden Dimensions: {config.get('hidden_dim', 'N/A')}\")\n        print(f\"  Max Grid Size: {config.get('max_grid_size', 'N/A')}\")\n        print(f\"  Relational Weight: {config.get('relational_weight', 'N/A')}\")\n        print(f\"  Pattern Memory Weight: {config.get('pattern_memory_weight', 'N/A')}\")\n        \nelse:\n    print(\"‚ùå MINERVA model not found. Training may still be in progress.\")\n\n# Check for training logs\niteration_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/minerva_specialized_iterations.json')\nif iteration_files:\n    with open(iteration_files[0], 'r') as f:\n        iterations = json.load(f)\n    \n    if iterations:\n        latest = iterations[-1]\n        print(f\"\\nüìä Latest MINERVA Training Iteration:\")\n        print(f\"  Iteration: {latest['iteration']}\")\n        print(f\"  Best Exact Match: {latest['results'].get('best_exact', 'N/A'):.2f}%\")\n        print(f\"  Best Validation Loss: {latest['results'].get('best_val_loss', 'N/A'):.4f}\")\n        print(f\"  Learning Rate Used: {latest['parameters'].get('learning_rate', 'N/A'):.4f}\")\n        print(f\"  Batch Size: {latest['parameters'].get('batch_size', 'N/A')}\")\n        print(f\"  Timestamp: {latest['timestamp']}\")\n\nprint(f\"\\nüîó Model Files Location: /content/AutomataNexus_Olympus_AGI2/arc_models_v4/\")\nprint(f\"üîó Training Logs: /content/AutomataNexus_Olympus_AGI2/arc_models_v4/\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Troubleshooting\n\nIf you encounter issues:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory if needed\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "print(f\"Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## MINERVA Specialized Training Complete!\n\nYour MINERVA model has been trained with its own specialized architecture-matched trainer and saved to `/content/AutomataNexus_Olympus_AGI2/arc_models_v4/`\n\n**MINERVA Specializations:**\n- ‚úÖ Grid Attention with learnable position embeddings\n- ‚úÖ Relational reasoning loss for spatial consistency  \n- ‚úÖ Pattern memory bank utilization optimization\n- ‚úÖ Object detection and boundary-aware loss\n- ‚úÖ Strategic analysis focused training\n- ‚úÖ ALL novel training methods integrated (MEPT, LEAP, PRISM, Exact Match Injection)\n- ‚úÖ MINERVA-specific DSL integration for deterministic grid pattern generation\n- ‚úÖ MINERVA-specific LEAP for grid-based pattern learning\n- ‚úÖ MINERVA-specific MEPT for spatial pattern memory\n- ‚úÖ MINERVA-specific PRISM for grid transformation synthesis\n\n**Model Files:**\n- `minerva_best.pt` - Best performing model checkpoint\n- `minerva_checkpoint.pt` - Latest training checkpoint\n\n**Next Steps:**\n- Create specialized trainers, DSL, LEAP, MEPT, and PRISM for ATLAS, IRIS, CHRONOS, PROMETHEUS\n- Each model needs architecture-matched training systems and pattern generation\n- Build final ensemble trainer after individual models are optimized\n- Test specialized models on ARC evaluation tasks\n\n**AutomataNexus OLYMPUS AGI2** - *Specialized Neural Architectures with Matched Training Methods*",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}