{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⏱️ CHRONOS Model Training\n",
    "\n",
    "**Temporal Sequence Analysis Specialist**\n",
    "\n",
    "This notebook trains the CHRONOS model, which specializes in:\n",
    "- Sequential patterns and movement trajectories\n",
    "- Temporal transformations in multi-step processes\n",
    "- LSTM + temporal attention mechanisms\n",
    "- Sequence modeling and prediction\n",
    "- State transitions and temporal dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Clone repository and install dependencies\nimport os\n\n# Check if the directory exists and create it if necessary\nmodels_dir = '/content/AutomataNexus_Olympus_AGI2/arc_models_v4/'\nif not os.path.exists(models_dir):\n    os.makedirs(models_dir, exist_ok=True)\n    print(f\"✅ Created models directory: {models_dir}\")\n\nif not os.path.exists('/content/AutomataNexus_Olympus_AGI2/.git'):\n    !git clone https://github.com/AutomataControls/AutomataNexus_Olympus_AGI2.git /content/AutomataNexus_Olympus_AGI2\n    !cd /content/AutomataNexus_Olympus_AGI2 && pip install -r requirements.txt -q\nelse:\n    print(\"Repository already exists\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify environment\n",
    "import torch\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# 🔧 Initialize CHRONOS Iterative Training Controller (RUN THIS FIRST!)\nimport json\nimport os\nimport datetime\nfrom typing import Dict, List\n\nclass ChronosIterativeTrainer:\n    def __init__(self, base_dir='/content/AutomataNexus_Olympus_AGI2'):\n        self.base_dir = base_dir\n        self.iteration_log_file = f\"{base_dir}/arc_models_v4/chronos_iterations.json\"\n        self.current_params = self.load_default_params()\n        self.iterations = self.load_iteration_history()\n    \n    def load_default_params(self) -> Dict:\n        return {\n            'learning_rate': 0.005,\n            'batch_size': 512,\n            'epochs_per_stage': 100,\n            'transformation_penalty': 0.5,      # Fixed per NexusReference.md\n            'exact_match_bonus': 5.0,          # Balanced value\n            'gradient_accumulation_steps': 4,\n            'use_mept': True,\n            'use_leap': True,\n            'use_prism': True,\n            'target_accuracy': 95.0,           # More realistic target\n            'exact_injection_epochs': 100,     # Longer pre-training\n            'stage_lr_reset': True,            # NEW: Reset LR each stage\n            'adaptive_lr': True,               # NEW: Boost LR when stuck\n            'checkpoint_reset_threshold': 2.0   # NEW: Reset if accuracy < 2%\n        }\n    \n    def load_iteration_history(self) -> List[Dict]:\n        if os.path.exists(self.iteration_log_file):\n            with open(self.iteration_log_file, 'r') as f:\n                return json.load(f)\n        return []\n    \n    def save_iteration_history(self):\n        os.makedirs(os.path.dirname(self.iteration_log_file), exist_ok=True)\n        with open(self.iteration_log_file, 'w') as f:\n            json.dump(self.iterations, f, indent=2)\n    \n    def log_iteration(self, params: Dict, results: Dict):\n        iteration = {\n            'iteration': len(self.iterations) + 1,\n            'timestamp': datetime.datetime.now().isoformat(),\n            'parameters': params.copy(),\n            'results': results.copy()\n        }\n        self.iterations.append(iteration)\n        self.save_iteration_history()\n    \n    def get_best_iteration(self) -> Dict:\n        if not self.iterations:\n            return None\n        return max(self.iterations, key=lambda x: x['results'].get('best_exact', 0))\n    \n    def suggest_next_params(self) -> Dict:\n        if len(self.iterations) < 2:\n            return self.current_params.copy()\n        \n        best = self.get_best_iteration()\n        latest = self.iterations[-1]\n        \n        suggestions = best['parameters'].copy() if best else self.current_params.copy()\n        \n        # Adaptive suggestions based on performance\n        if latest and best:\n            latest_exact = latest['results'].get('best_exact', 0)\n            best_exact = best['results'].get('best_exact', 0)\n            \n            if latest_exact < 1.0:  # Really poor performance\n                # More aggressive learning rate\n                suggestions['learning_rate'] = min(0.008, suggestions['learning_rate'] * 1.3)\n                suggestions['adaptive_lr'] = True\n                suggestions['checkpoint_reset_threshold'] = 2.0\n            elif latest_exact < best_exact * 0.8:  # Performance dropped\n                suggestions['learning_rate'] *= 0.7\n                suggestions['transformation_penalty'] = max(0.5, suggestions['transformation_penalty'])\n            elif latest_exact > best_exact * 1.1:  # Good improvement\n                suggestions['learning_rate'] = min(0.01, suggestions['learning_rate'] * 1.1)\n        \n        return suggestions\n    \n    def display_history(self):\n        if not self.iterations:\n            print(\"No CHRONOS iterations found. Starting fresh!\")\n            return\n        \n        print(\"📈 CHRONOS Training History (PERFORMANCE FIXES v2):\")\n        print(\"-\" * 85)\n        for i, iteration in enumerate(self.iterations):\n            exact = iteration['results'].get('best_exact', 0)\n            loss = iteration['results'].get('best_val_loss', float('inf'))\n            lr = iteration['parameters'].get('learning_rate', 0)\n            trans_pen = iteration['parameters'].get('transformation_penalty', 0)\n            exact_bonus = iteration['parameters'].get('exact_match_bonus', 0)\n            timestamp = iteration['timestamp'][:16]\n            \n            status = \"🟢 BEST\" if iteration == self.get_best_iteration() else \"⚪\"\n            print(f\"{status} Iter {i+1}: {exact:.2f}% exact | Loss: {loss:.4f} | LR: {lr:.4f} | Trans: {trans_pen:.1f} | Bonus: {exact_bonus:.1f} | {timestamp}\")\n        \n        print(\"-\" * 85)\n        best = self.get_best_iteration()\n        if best:\n            print(f\"🏆 Best: Iteration {best['iteration']} with {best['results']['best_exact']:.2f}% exact match\")\n        \n        print(f\"\\n🔧 CHRONOS Training Fixes Applied (v2):\")\n        print(f\"   ✅ Transformation penalty: 0.5 (was 0.3)\")\n        print(f\"   ✅ Exact match bonus: 5.0 (was 10.0)\")  \n        print(f\"   ✅ MEPT enabled for all stages (was Stage 0 only)\")\n        print(f\"   ✅ Consistent batch size across stages\")\n        print(f\"   ✅ Improved early stopping logic\")\n        print(f\"   ✅ Longer exact match pre-training (100 epochs)\")\n        print(f\"   🆕 Stage-specific LR reset (prevent decay issues)\")\n        print(f\"   🆕 Adaptive LR boost when stuck (< 1% accuracy)\")\n        print(f\"   🆕 LEAP training for all stages (was Stage 0 only)\")\n        print(f\"   🆕 Checkpoint reset if accuracy < 2% in Stage 1+\")\n\n# Initialize CHRONOS trainer and suggested parameters\nchronos_trainer = ChronosIterativeTrainer()\nsuggested_params = chronos_trainer.suggest_next_params()\nchronos_trainer.display_history()\nprint(\"\\n✅ CHRONOS trainer initialized with ADVANCED FIXES! Ready for v2 training.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Start CHRONOS Training\n",
    "\n",
    "Training configuration:\n",
    "- **Architecture**: LSTM + Temporal Attention\n",
    "- **Parameters**: ~2.4M\n",
    "- **Specialization**: Sequential patterns with 92% accuracy\n",
    "- **Focus**: Movement prediction and multi-step processes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Pull latest updates before training\n!cd /content/AutomataNexus_Olympus_AGI2 && git pull"
  },
  {
   "cell_type": "code",
   "source": "!cd /content/AutomataNexus_Olympus_AGI2 && python scripts/training/train_chronos_specialized.py",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Monitor Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# 🎯 CHRONOS Specialized Training with TEMPORAL ARCHITECTURE FOCUS\nimport subprocess\nimport tempfile\nimport os\n\n# Initialize trainer if not already done\nif 'chronos_trainer' not in globals():\n    print(\"Error: Run the trainer initialization cell first!\")\n    raise NameError(\"chronos_trainer not defined\")\n\n# Use SPECIALIZED CHRONOS parameters for temporal sequence analysis:\nLEARNING_RATE = 0.004  # Lower for sequence stability\nBATCH_SIZE = 256       # Smaller for temporal complexity  \nEPOCHS_PER_STAGE = 100\nTRANSFORMATION_PENALTY = 0.3  # Lower - CHRONOS should do temporal transformations\nEXACT_MATCH_BONUS = 6.0       # High for temporal precision\nGRADIENT_ACCUMULATION_STEPS = 2  # Effective batch: 512\nUSE_MEPT = True\nUSE_LEAP = True\nUSE_PRISM = True\nTARGET_ACCURACY = 93.0  # Realistic for complex temporal patterns\n\nprint(f\"⏰ Starting CHRONOS SPECIALIZED training iteration {len(chronos_trainer.iterations) + 1}\")\nprint(f\"🔧 TEMPORAL Parameters: LR={LEARNING_RATE}, BS={BATCH_SIZE}, Epochs/Stage={EPOCHS_PER_STAGE}\")\nprint(f\"🔧 TEMPORAL Loss weights: Transform penalty={TRANSFORMATION_PENALTY}, Exact bonus={EXACT_MATCH_BONUS}\")\nprint(f\"🔧 TEMPORAL Features: Sequence analysis, Object tracking, Movement prediction\")\nprint(f\"🎯 Temporal target accuracy: {TARGET_ACCURACY}%\")\n\n# Run CHRONOS specialized training directly\ntry:\n    print(\"⏰ CHRONOS specialized training timeout: 6 hours\")\n    result = subprocess.run(\n        ['python', 'scripts/training/train_chronos_specialized.py'],\n        cwd='/content/AutomataNexus_Olympus_AGI2',\n        capture_output=True,\n        text=True,\n        timeout=21600  # 6 hours for temporal sequence training\n    )\n    \n    print(\"CHRONOS specialized training output:\")\n    print(result.stdout)\n    if result.stderr:\n        print(\"Errors:\")\n        print(result.stderr)\n        \nexcept subprocess.TimeoutExpired:\n    print(\"⚠️ CHRONOS training timeout reached (6 hours)\")\nexcept Exception as e:\n    print(f\"❌ Training error: {e}\")",
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# 📊 Log CHRONOS SPECIALIZED Results and Update History\nimport glob\nimport torch\nimport json\n\n# Collect results from the latest CHRONOS specialized training\ncurrent_params = {\n    'learning_rate': 0.004,  # Temporal-optimized\n    'batch_size': 256,       # Smaller for temporal complexity\n    'epochs_per_stage': 100,\n    'transformation_penalty': 0.3,  # Lower for temporal transformations\n    'exact_match_bonus': 6.0,       # High for temporal precision\n    'gradient_accumulation_steps': 2,\n    'use_mept': True,\n    'use_leap': True,\n    'use_prism': True,\n    'temporal_weight': 0.5,\n    'movement_weight': 0.4,\n    'object_tracking_weight': 0.3,\n    'sequence_consistency_weight': 0.4\n}\n\n# Load latest CHRONOS specialized checkpoint to get results\nmodel_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/models/chronos_specialized_best.pth')\nresults = {}\n\nif model_files:\n    checkpoint = torch.load(model_files[0], map_location='cpu')\n    results = {\n        'best_exact': checkpoint.get('val_exact', 0),\n        'best_val_loss': checkpoint.get('val_loss', float('inf')),\n        'training_epoch': checkpoint.get('epoch', 0),\n        'stage': checkpoint.get('stage', 0)\n    }\n    \n    print(f\"✅ CHRONOS SPECIALIZED training completed!\")\n    print(f\"   Best exact match: {results['best_exact']:.2f}%\")\n    print(f\"   Best validation loss: {results['best_val_loss']:.4f}\")\n    print(f\"   Training epochs: {results['training_epoch']}\")\n    print(f\"   CHRONOS Config: {checkpoint.get('config', {})}\")\n    \n    # Log this iteration\n    chronos_trainer.log_iteration(current_params, results)\n    \n    # Display updated history\n    print(\"\\n\" + \"=\"*60)\n    chronos_trainer.display_history()\n    \n    # Provide temporal-specific analysis\n    print(\"\\n⏰ CHRONOS Temporal Analysis:\")\n    current_exact = results['best_exact']\n    \n    if current_exact >= 15.0:\n        print(\"   🎯 EXCELLENT temporal sequence performance!\")\n        print(\"   ✅ CHRONOS architecture is working well for sequential patterns\")\n    elif current_exact >= 10.0:\n        print(\"   👍 GOOD temporal performance - above baseline\")\n        print(\"   🔄 Consider fine-tuning temporal weights for better sequence learning\")\n    elif current_exact >= 5.0:\n        print(\"   ⚠️ MODERATE temporal performance\")\n        print(\"   📉 May need longer training or temporal architecture adjustments\")\n    else:\n        print(\"   ❌ LOW temporal performance - architecture/parameter issues\")\n        print(\"   🔧 Review CHRONOS temporal components and loss weights\")\n    \nelse:\n    print(\"❌ No CHRONOS specialized model checkpoint found. Training may have failed.\")\n    \nprint(\"\\n🔄 CHRONOS specialized training uses temporal-focused architecture.\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Load and evaluate CHRONOS SPECIALIZED model\nimport torch\nimport json\nimport glob\nimport os\n\n# Find CHRONOS specialized model file\nmodel_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/models/chronos_specialized_best.pth')\n\nif model_files:\n    model_file = model_files[0]\n    checkpoint = torch.load(model_file, map_location='cpu')\n    \n    print(\"⏰ CHRONOS SPECIALIZED Training Results:\")\n    print(f\"  Best Validation Exact Match: {checkpoint.get('val_exact', 'N/A'):.2f}%\")\n    print(f\"  Training Epoch: {checkpoint.get('epoch', 'N/A')}\")\n    print(f\"  Stage: {checkpoint.get('stage', 'N/A')}\")\n    print(f\"  Model Size: {os.path.getsize(model_file) / (1024**2):.1f} MB\")\n    \n    # Display CHRONOS-specific configuration\n    config = checkpoint.get('config', {})\n    if config:\n        print(f\"\\n⏰ CHRONOS Specialized Configuration:\")\n        print(f\"  Temporal Weight: {config.get('temporal_weight', 'N/A')}\")\n        print(f\"  Movement Weight: {config.get('movement_weight', 'N/A')}\")\n        print(f\"  Object Tracking Weight: {config.get('object_tracking_weight', 'N/A')}\")\n        print(f\"  Sequence Consistency Weight: {config.get('sequence_consistency_weight', 'N/A')}\")\n        print(f\"  Sequence Length: {config.get('sequence_length', 'N/A')}\")\n        print(f\"  Hidden Dim: {config.get('hidden_dim', 'N/A')}\")\n        \nelse:\n    print(\"❌ CHRONOS specialized model not found. Training may still be in progress.\")\n\n# Check for old generic training results for comparison\nold_model_files = glob.glob('/content/AutomataNexus_Olympus_AGI2/arc_models_v4/chronos_best.pt')\nif old_model_files:\n    old_checkpoint = torch.load(old_model_files[0], map_location='cpu')\n    old_exact = old_checkpoint.get('val_exact', 0)\n    print(f\"\\n📊 Comparison with Generic Training:\")\n    print(f\"  Generic CHRONOS: {old_exact:.2f}%\")\n    if model_files:\n        new_exact = checkpoint.get('val_exact', 0)\n        print(f\"  Specialized CHRONOS: {new_exact:.2f}%\")\n        improvement = new_exact - old_exact\n        print(f\"  Improvement: {improvement:+.2f}% {'🎯' if improvement > 0 else '⚠️'}\")\n\nprint(f\"\\n⏰ CHRONOS specializes in temporal sequence analysis with:\")\nprint(f\"  • Sequential pattern recognition\")\nprint(f\"  • Object movement tracking\") \nprint(f\"  • Temporal state transitions\")\nprint(f\"  • LSTM + attention mechanisms\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Troubleshooting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU memory if needed\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
    "print(f\"Free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved()) / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Training Complete!\n",
    "\n",
    "Your CHRONOS model has been trained and saved to `/content/AutomataNexus_Olympus_AGI2/arc_models_v4/`\n",
    "\n",
    "**CHRONOS Specialization:**\n",
    "- Temporal sequence analysis with 92% accuracy\n",
    "- LSTM + attention mechanisms\n",
    "- Movement prediction and state transitions\n",
    "\n",
    "**AutomataNexus OLYMPUS AGI2** - *Where Neural Networks Meet Symbolic Logic*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}