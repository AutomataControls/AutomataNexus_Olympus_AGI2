"""
MINERVA V3 Training Script - Enhanced Strategic Grid Analysis with Program Synthesis
Building on V2's 55.62% performance with sophisticated ARC reasoning
Target: 60%+ performance with enhanced program synthesis capabilities
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, random_split
from torch.amp import GradScaler, autocast
import numpy as np
import json
import os
import sys
import gc
import time
from tqdm import tqdm
from typing import Dict, List, Optional, Tuple
import random
from collections import defaultdict
import math

# Add project paths
sys.path.append('/content/AutomataNexus_Olympus_AGI2')
sys.path.append('/content/AutomataNexus_Olympus_AGI2/src')
sys.path.append('/content/AutomataNexus_Olympus_AGI2/scripts/training')

# Import MINERVA model
from src.models.minerva_model import EnhancedMinervaNet

# Enhanced MINERVA Configuration V3 - Complex Strategic Training
MINERVA_CONFIG = {
    # Core Training Parameters - Slower, More Careful Learning
    'batch_size': 32,  # Smaller for complex patterns
    'learning_rate': 0.0003,  # Lower LR for careful learning
    'num_epochs': 500,  # 10 stages x 50 epochs
    'gradient_accumulation': 8,  # Effective batch: 256
    'epochs_per_stage': 50,  # Extended epochs per stage
    'curriculum_stages': 10,  # Extended to 10 stages
    
    # Enhanced Loss Configuration
    'transform_penalty': 0.5,  # Keep positive as required
    'exact_match_bonus': 6.0,  # Higher bonus for exact matches
    'gradient_clip': 0.8,  # Tighter clipping for stability
    'weight_decay': 5e-6,  # Very light regularization
    
    # PROMETHEUS-Style Enhancements
    'ultra_teal_iou_weight': 0.85,  # 85% IoU weighting
    'strict_match_weight': 0.15,   # 15% strict matching
    'creativity_weight': 0.25,     # Enhanced creativity bonus
    'strategic_planning_weight': 0.2,  # Strategic planning bonus
    'multi_step_reasoning_weight': 0.15,  # Multi-step reasoning bonus
    
    # Advanced Training Features
    'label_smoothing': 0.05,  # Light smoothing for generalization
    'pattern_diversity_bonus': True,
    'abstract_reasoning_bonus': True,
    'meta_learning_enabled': True,
    'advanced_augmentation': True,
    
    # Learning Rate Scheduling
    'warmup_epochs': 25,  # Extended warmup
    'cosine_restarts': True,
    'restart_multiplier': 1.2,
}

# Enhanced 10-Stage Progressive Configuration - 6x6 to 35x35
STAGE_CONFIG = [
    # Basic Strategic Patterns
    {'stage': 0, 'max_grid_size': 6,  'synthesis_ratio': 0.9, 'pattern_complexity': 'basic_strategic'},
    {'stage': 1, 'max_grid_size': 8,  'synthesis_ratio': 0.8, 'pattern_complexity': 'simple_logic'},
    {'stage': 2, 'max_grid_size': 10, 'synthesis_ratio': 0.7, 'pattern_complexity': 'pattern_completion'},
    
    # Intermediate Strategic Reasoning
    {'stage': 3, 'max_grid_size': 12, 'synthesis_ratio': 0.6, 'pattern_complexity': 'multi_step_basic'},
    {'stage': 4, 'max_grid_size': 15, 'synthesis_ratio': 0.5, 'pattern_complexity': 'symmetry_advanced'},
    {'stage': 5, 'max_grid_size': 18, 'synthesis_ratio': 0.4, 'pattern_complexity': 'sequence_patterns'},
    
    # Advanced Abstract Reasoning
    {'stage': 6, 'max_grid_size': 22, 'synthesis_ratio': 0.3, 'pattern_complexity': 'multi_step_logical'},
    {'stage': 7, 'max_grid_size': 26, 'synthesis_ratio': 0.2, 'pattern_complexity': 'abstract_completion'},
    {'stage': 8, 'max_grid_size': 30, 'synthesis_ratio': 0.15, 'pattern_complexity': 'complex_spatial'},
    {'stage': 9, 'max_grid_size': 35, 'synthesis_ratio': 0.1, 'pattern_complexity': 'expert_reasoning'}
]

# Device setup
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print("=" * 80)
print("MINERVA Enhanced V3 Training - Advanced Strategic Grid Analysis")
print("Building on V2's 55.62% ‚Üí Target: 60%+")
print("=" * 80)


class EnhancedMinervaLoss(nn.Module):
    """Enhanced MINERVA loss with advanced program synthesis and strategic reasoning"""
    
    def __init__(self, transformation_penalty=0.5, exact_match_bonus=6.0):
        super().__init__()
        self.transformation_penalty = transformation_penalty
        self.exact_match_bonus = exact_match_bonus
        self.ultra_teal_iou_weight = MINERVA_CONFIG['ultra_teal_iou_weight']
        self.strict_match_weight = MINERVA_CONFIG['strict_match_weight']
        
    def forward(self, model_outputs, targets, inputs, mixup_lambda=None):
        """Enhanced forward pass with mixup handling and strategic bonuses"""
        
        # Handle mixup targets if provided
        if mixup_lambda is not None and isinstance(targets, tuple):
            targets_a, targets_b = targets
            loss_a = self._calculate_single_loss(model_outputs, targets_a, inputs)
            loss_b = self._calculate_single_loss(model_outputs, targets_b, inputs)
            
            # Mix the losses
            mixed_losses = {}
            for key in loss_a:
                if torch.is_tensor(loss_a[key]):
                    mixed_losses[key] = mixup_lambda * loss_a[key] + (1 - mixup_lambda) * loss_b[key]
                else:
                    mixed_losses[key] = loss_a[key]
            
            return mixed_losses
        
        return self._calculate_single_loss(model_outputs, targets, inputs)
    
    def _calculate_single_loss(self, model_outputs, targets, inputs):
        """Calculate enhanced loss with program synthesis and strategic reasoning"""
        pred_output = model_outputs['predicted_output']
        B, C, H, W = pred_output.shape
        
        # Apply label smoothing for better generalization
        if MINERVA_CONFIG.get('label_smoothing', 0) > 0:
            targets = self._apply_label_smoothing(targets, MINERVA_CONFIG['label_smoothing'])
        
        # Enhanced focal loss with strategic pattern weighting
        focal_loss = self._enhanced_focal_loss(pred_output, targets, gamma=2.5)
        
        # ULTRA TEAL exact match scoring (85% IoU + 15% strict)
        pred_indices = pred_output.argmax(dim=1)
        target_indices = targets.argmax(dim=1) if targets.dim() > 3 else targets
        
        # Strict exact matches
        exact_matches_strict = (pred_indices == target_indices).all(dim=[1,2]).float()
        
        # IoU-based soft exact match
        intersection = (pred_indices == target_indices).float().sum(dim=[1,2])
        union = torch.full_like(intersection, H * W)
        iou_scores = intersection / union
        
        # ULTRA TEAL: 85% IoU + 15% strict matching for maximum soft matching
        combined_matches = self.strict_match_weight * exact_matches_strict + self.ultra_teal_iou_weight * iou_scores
        exact_count = combined_matches.sum()
        exact_bonus = -combined_matches.mean() * self.exact_match_bonus
        exact_bonus = exact_bonus.clamp(min=-4.0)  # More aggressive than standard
        
        # Enhanced transformation penalty with strategic logic focus
        input_indices = inputs.argmax(dim=1) if inputs.dim() > 3 else inputs
        copy_penalty = (pred_indices == input_indices).all(dim=[1,2]).float()
        transform_penalty = copy_penalty.mean() * self.transformation_penalty
        
        # Strategic planning bonus
        strategic_planning_bonus = 0.0
        if 'features' in model_outputs and MINERVA_CONFIG.get('strategic_planning_weight', 0) > 0:
            features = model_outputs['features']
            # Reward complex feature representations - handle tensor shapes safely
            if features.numel() > 0:
                flattened = features.reshape(B, -1)
                if flattened.shape[1] > 0:
                    feature_complexity = torch.std(flattened, dim=1).mean()
                    strategic_planning_bonus = feature_complexity * MINERVA_CONFIG['strategic_planning_weight'] * 0.01
        
        # Multi-step reasoning bonus
        multi_step_bonus = 0.0
        if 'transform_params' in model_outputs and MINERVA_CONFIG.get('multi_step_reasoning_weight', 0) > 0:
            transform_params = model_outputs['transform_params']
            # Reward diverse transformation parameters - handle tensor shapes safely
            if transform_params.numel() > 0:
                flattened = transform_params.reshape(B, -1)
                if flattened.shape[1] > 0:
                    transform_diversity = torch.std(flattened, dim=1).mean()
                    multi_step_bonus = transform_diversity * MINERVA_CONFIG['multi_step_reasoning_weight'] * 0.01
        
        # Pattern diversity bonus
        diversity_bonus = 0.0
        if MINERVA_CONFIG.get('pattern_diversity_bonus'):
            diversity_bonus = self._calculate_pattern_diversity(pred_indices)
        
        # Abstract reasoning complexity bonus for larger grids
        complexity_bonus = 0.0
        if H > 15:  # Reward performance on complex grids
            grid_complexity_factor = min((H * W) / 1225.0, 1.0)  # Normalize by 35x35
            complexity_bonus = combined_matches.mean() * grid_complexity_factor * 0.08
        
        # Enhanced creativity bonus
        creativity_bonus = 0.0
        if MINERVA_CONFIG.get('creativity_weight', 0) > 0:
            # Reward predictions that are different from input but match target
            input_output_diff = (pred_indices != input_indices).float().mean()
            target_match = combined_matches.mean()
            creativity_bonus = input_output_diff * target_match * MINERVA_CONFIG['creativity_weight']
        
        # Total enhanced loss
        total_loss = (focal_loss + transform_penalty + exact_bonus - 
                     strategic_planning_bonus - multi_step_bonus - diversity_bonus - 
                     complexity_bonus - creativity_bonus)
        
        # Stability check
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            print(f"‚ö†Ô∏è NaN/Inf loss in MINERVA V3, using focal only")
            total_loss = focal_loss.clamp(max=10.0)
        
        return {
            'total': total_loss,
            'focal': focal_loss,
            'transform': transform_penalty,
            'exact_bonus': exact_bonus,
            'exact_count': exact_count,
            'ultra_teal_count': combined_matches.sum(),
            'avg_iou': iou_scores.mean(),
            'strategic_bonus': strategic_planning_bonus,
            'multi_step_bonus': multi_step_bonus,
            'diversity_bonus': diversity_bonus,
            'complexity_bonus': complexity_bonus,
            'creativity_bonus': creativity_bonus,
        }
    
    def _apply_label_smoothing(self, targets, smoothing):
        """Apply label smoothing for better generalization"""
        if targets.dim() == 3:  # Convert indices to one-hot if needed
            targets = F.one_hot(targets, num_classes=10).permute(0, 3, 1, 2).float()
        
        C = targets.shape[1]
        smooth_targets = targets * (1 - smoothing) + smoothing / C
        return smooth_targets
    
    def _enhanced_focal_loss(self, pred, target, gamma=2.5):
        """Enhanced focal loss with strategic pattern emphasis"""
        target_idx = target.argmax(dim=1) if target.dim() > 3 else target
        ce_loss = F.cross_entropy(pred, target_idx, reduction='none')
        
        # Enhanced strategic weighting
        pt = torch.exp(-ce_loss)
        strategic_weights = torch.ones_like(ce_loss)
        
        # Weight based on pattern complexity and strategic elements
        for b in range(pred.shape[0]):
            grid = target_idx[b]
            H, W = grid.shape
            
            # Complex pattern bonus (more unique colors)
            unique_colors = torch.unique(grid).numel()
            if unique_colors > 3:
                strategic_weights[b] *= 1.3
            
            # Spatial complexity bonus (more varied local patterns)
            if H >= 8 and W >= 8:
                local_variance = 0
                for i in range(0, H-2, 2):
                    for j in range(0, W-2, 2):
                        local_patch = grid[i:i+3, j:j+3]
                        local_variance += torch.unique(local_patch).numel()
                
                avg_local_complexity = local_variance / ((H//2) * (W//2))
                if avg_local_complexity > 2.5:
                    strategic_weights[b] *= 1.2
        
        focal = (1 - pt) ** gamma * ce_loss * strategic_weights
        return focal.mean()
    
    def _calculate_pattern_diversity(self, pred_indices):
        """Calculate pattern diversity bonus for enhanced strategic reasoning"""
        diversity_scores = []
        B = pred_indices.shape[0]
        
        for b in range(B):
            grid = pred_indices[b]
            H, W = grid.shape
            
            # Count unique local patterns (3x3 neighborhoods)
            patterns = set()
            for i in range(H-2):
                for j in range(W-2):
                    pattern = tuple(grid[i:i+3, j:j+3].flatten().tolist())
                    patterns.add(pattern)
            
            # Normalize by possible positions
            max_patterns = (H-2) * (W-2)
            if max_patterns > 0:
                diversity_score = len(patterns) / max_patterns
                diversity_scores.append(torch.tensor(diversity_score, device=pred_indices.device))
        
        if diversity_scores:
            return torch.stack(diversity_scores).mean() * 0.03
        return torch.tensor(0.0, device=pred_indices.device)


def mixup_data(x, y, alpha=0.3):
    """Enhanced mixup augmentation for complex pattern learning"""
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1
    
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    
    mixed_x = lam * x + (1 - lam) * x[index]
    y_a, y_b = y, y[index]
    
    return mixed_x, (y_a, y_b), lam


def advanced_strategic_augmentation(inputs, outputs):
    """Advanced augmentation for enhanced ARC pattern learning"""
    if random.random() < 0.4:  # Increased probability for more diversity
        # Strategic rotation with pattern preservation
        k = random.choice([1, 2, 3])  # 90, 180, 270 degrees
        inputs = torch.rot90(inputs, k, dims=[-2, -1])
        outputs = torch.rot90(outputs, k, dims=[-2, -1])
    
    if random.random() < 0.3:
        # Strategic flip (maintain logical consistency)
        if random.random() < 0.5:
            inputs = torch.flip(inputs, dims=[-1])  # Horizontal
            outputs = torch.flip(outputs, dims=[-1])
        else:
            inputs = torch.flip(inputs, dims=[-2])  # Vertical
            outputs = torch.flip(outputs, dims=[-2])
    
    # Advanced noise injection for robustness (very light)
    if random.random() < 0.1:
        noise_mask = torch.rand_like(inputs.float()) < 0.02  # 2% of pixels
        if noise_mask.any():
            noise_values = torch.randint(0, 10, inputs.shape, device=inputs.device)
            inputs = torch.where(noise_mask, noise_values, inputs)
    
    return inputs, outputs


def custom_collate_fn(batch):
    """Enhanced collate function with padding for variable sizes"""
    # Handle batch items that might be tuples or dicts
    if isinstance(batch[0], tuple):
        # Convert tuples to dicts
        batch = [{'inputs': item[0], 'outputs': item[1]} for item in batch]
    
    # Find maximum dimensions - handle different tensor shapes
    max_h = 0
    max_w = 0
    
    for item in batch:
        inp = item['inputs']
        # Handle different tensor shapes
        if inp.dim() == 2:  # H, W
            h, w = inp.shape
        elif inp.dim() == 3:  # C, H, W or H, W, C
            if inp.shape[0] <= 10:  # Likely C, H, W
                h, w = inp.shape[1], inp.shape[2]
            else:  # Likely H, W, C
                h, w = inp.shape[0], inp.shape[1]
        else:
            continue
        
        max_h = max(max_h, h)
        max_w = max(max_w, w)
    
    # Pad all tensors to maximum size
    padded_inputs = []
    padded_outputs = []
    
    for item in batch:
        inp = item['inputs']
        out = item['outputs']
        
        # Ensure consistent format (H, W)
        if inp.dim() == 3:
            if inp.shape[0] <= 10:  # C, H, W -> H, W
                inp = inp.argmax(dim=0)
            else:  # H, W, C -> H, W
                inp = inp.argmax(dim=-1)
        
        if out.dim() == 3:
            if out.shape[0] <= 10:  # C, H, W -> H, W
                out = out.argmax(dim=0)
            else:  # H, W, C -> H, W
                out = out.argmax(dim=-1)
        
        # Pad to maximum size
        h_pad = max_h - inp.shape[0]
        w_pad = max_w - inp.shape[1]
        
        if h_pad > 0 or w_pad > 0:
            inp = F.pad(inp, (0, w_pad, 0, h_pad), mode='constant', value=0)
            out = F.pad(out, (0, w_pad, 0, h_pad), mode='constant', value=0)
        
        padded_inputs.append(inp)
        padded_outputs.append(out)
    
    return {
        'inputs': torch.stack(padded_inputs),
        'outputs': torch.stack(padded_outputs)
    }


def train_minerva_specialized_v3():
    """Enhanced MINERVA V3 training with program synthesis and advanced reasoning"""
    print("üß† Starting MINERVA V3 Enhanced Training")
    print("=" * 70)
    print("üìä Advanced Strategic Grid Analysis Features:")
    print("  ‚Ä¢ 10-stage progressive curriculum (6x6 ‚Üí 35x35)")
    print("  ‚Ä¢ Enhanced program synthesis capabilities")
    print("  ‚Ä¢ ULTRA TEAL IoU scoring (85% soft + 15% strict)")
    print("  ‚Ä¢ Advanced pattern diversity and complexity bonuses")
    print("  ‚Ä¢ Strategic planning and multi-step reasoning")
    print("  ‚Ä¢ Extended 500-epoch training with careful learning")
    print("=" * 70)
    
    # Initialize enhanced model
    model = EnhancedMinervaNet(max_grid_size=35).to(device)
    print(f"üß† MINERVA V3 Model: {sum(p.numel() for p in model.parameters()):,} parameters")
    
    # Enhanced loss function
    loss_fn = EnhancedMinervaLoss(
        transformation_penalty=MINERVA_CONFIG['transform_penalty'],
        exact_match_bonus=MINERVA_CONFIG['exact_match_bonus']
    ).to(device)
    
    # Enhanced optimizer with very careful learning rate
    optimizer = optim.AdamW(
        model.parameters(),
        lr=MINERVA_CONFIG['learning_rate'],
        betas=(0.9, 0.999),
        weight_decay=MINERVA_CONFIG['weight_decay']
    )
    
    # Advanced scheduler with warm restarts
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, 
        T_0=MINERVA_CONFIG['epochs_per_stage'],
        T_mult=int(MINERVA_CONFIG['restart_multiplier']),
        eta_min=MINERVA_CONFIG['learning_rate'] * 0.05
    )
    
    # Mixed precision
    scaler = GradScaler('cuda' if device.type == 'cuda' else 'cpu')
    
    # Model directory
    models_dir = '/mnt/d/opt/AutomataNexus_Olympus_AGI2/arc_models_v4'
    os.makedirs(models_dir, exist_ok=True)
    best_model_path = f'{models_dir}/minerva_v3_enhanced_best.pt'
    
    best_exact = 0.0
    global_epoch = 0
    start_stage = 0
    
    # Load existing best model if available
    if os.path.exists(best_model_path):
        print(f"üîÑ Loading best MINERVA V3 model from {best_model_path}")
        try:
            checkpoint = torch.load(best_model_path, map_location=device)
            model.load_state_dict(checkpoint['model_state_dict'])
            if 'optimizer_state_dict' in checkpoint:
                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            if 'scheduler_state_dict' in checkpoint:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            best_exact = checkpoint.get('best_exact', 0.0)
            global_epoch = checkpoint.get('epoch', 0)
            start_stage = checkpoint.get('stage', 0)
            print(f"‚úÖ Resumed from epoch {global_epoch}, stage {start_stage}, best: {best_exact:.2f}%")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to load checkpoint: {e}")
    else:
        print("üÜï No existing model found - starting fresh V3 training")
    
    # Data directory
    DATA_DIR = '/content/AutomataNexus_Olympus_AGI2/data'
    
    # Import dataset components
    sys.path.append('/content/AutomataNexus_Olympus_AGI2/scripts/training')
    try:
        from colab_training_v4_megascale_curriculum import CurriculumMegaScaleDataset
        dataset_available = True
    except ImportError:
        print("‚ö†Ô∏è Dataset not available - using fallback")
        dataset_available = False
        return None, 0.0
    
    print(f"\nüß† MINERVA V3 10-Stage Progressive Enhanced Training")
    print("=" * 70)
    
    # Enhanced stage tracking
    stage_results = {}
    
    # 10-Stage Progressive Training with Enhanced Features
    for stage in range(start_stage, MINERVA_CONFIG['curriculum_stages']):
        stage_config = STAGE_CONFIG[stage]
        grid_size = stage_config['max_grid_size']
        synthesis_ratio = stage_config['synthesis_ratio']
        pattern_complexity = stage_config['pattern_complexity']
        
        print(f"\nüß† MINERVA V3 Stage {stage}: {grid_size}x{grid_size} Enhanced Strategic Analysis")
        print(f"   üìè Grid Size: {grid_size}x{grid_size} | Synthesis: {synthesis_ratio*100:.0f}%")
        print(f"   üéØ Pattern Complexity: {pattern_complexity} | Focus: Program synthesis")
        print("=" * 60)
        
        # Create enhanced dataset
        try:
            dataset = CurriculumMegaScaleDataset(
                DATA_DIR,
                curriculum_stage=min(stage, 7),  # Cap at available stages
                use_arc_synthesis=True,
                synthesis_ratio=synthesis_ratio
            )
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to create dataset: {e}")
            continue
        
        # Split dataset with more training data
        train_size = int(0.88 * len(dataset))  # More training for complex patterns
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
        
        train_loader = DataLoader(
            train_dataset,
            batch_size=MINERVA_CONFIG['batch_size'],
            shuffle=True,
            num_workers=0,
            pin_memory=True,
            collate_fn=custom_collate_fn,
            drop_last=True
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=MINERVA_CONFIG['batch_size'],
            shuffle=False,
            num_workers=0,
            pin_memory=True,
            collate_fn=custom_collate_fn,
            drop_last=False
        )
        
        print(f"üìö Stage {stage} ({grid_size}x{grid_size}) - Train: {len(train_dataset):,}, Val: {len(val_dataset):,}")
        
        # Enhanced exact match injection for early stages
        if stage <= 2:  # Apply to first 3 stages
            print(f"üéØ Enhanced Strategic Pattern Injection for Stage {stage}")
            try:
                for epoch in range(40):  # Extended injection period
                    model.train()
                    injection_exact = 0
                    injection_total = 0
                    
                    # Create more sophisticated patterns
                    for _ in range(150):  # More patterns
                        size = random.choice([min(grid_size, 8), min(grid_size, 10)])
                        
                        # Advanced pattern generation
                        if pattern_complexity == 'basic_strategic':
                            # Symmetry and simple transformations
                            input_grid = torch.zeros(size, size, dtype=torch.long)
                            for i in range(size//2):
                                for j in range(size//2):
                                    color = random.randint(1, 4)
                                    input_grid[i, j] = color
                                    input_grid[size-1-i, j] = color
                                    input_grid[i, size-1-j] = color
                                    input_grid[size-1-i, size-1-j] = color
                            output_grid = input_grid.clone()
                            
                        elif pattern_complexity == 'simple_logic':
                            # Color mappings and rotations
                            input_grid = torch.randint(1, 5, (size, size))
                            if random.random() < 0.5:
                                # Color transformation
                                output_grid = input_grid.clone()
                                for old_color in [1, 2, 3, 4]:
                                    new_color = random.randint(1, 4)
                                    output_grid[input_grid == old_color] = new_color
                            else:
                                # Rotation
                                output_grid = torch.rot90(input_grid, k=random.choice([1, 2, 3]))
                                
                        else:
                            # Pattern completion
                            input_grid = torch.randint(1, 5, (size, size))
                            output_grid = input_grid.clone()
                            # Add systematic pattern
                            for i in range(0, size, 2):
                                for j in range(0, size, 2):
                                    if i < size and j < size:
                                        output_grid[i, j] = (input_grid[i, j] % 4) + 1
                        
                        # Train on pattern
                        optimizer.zero_grad()
                        
                        inp_oh = F.one_hot(input_grid.to(device).unsqueeze(0), num_classes=10).permute(0, 3, 1, 2).float()
                        out_oh = F.one_hot(output_grid.to(device).unsqueeze(0), num_classes=10).permute(0, 3, 1, 2).float()
                        
                        model_outputs = model(inp_oh, out_oh, mode='train')
                        losses = loss_fn(model_outputs, out_oh, inp_oh)
                        
                        losses['total'].backward()
                        torch.nn.utils.clip_grad_norm_(model.parameters(), MINERVA_CONFIG['gradient_clip'])
                        optimizer.step()
                        
                        # Check exact match
                        pred_idx = model_outputs['predicted_output'].argmax(dim=1)
                        exact_match = (pred_idx[0] == output_grid.to(device)).all()
                        injection_exact += int(exact_match)
                        injection_total += 1
                    
                    injection_accuracy = injection_exact / injection_total * 100
                    if epoch % 10 == 0:
                        print(f"Strategic Injection Epoch {epoch+1}/40: {injection_accuracy:.1f}% accuracy")
                    
                    if injection_accuracy >= 88.0:  # Higher target
                        print(f"‚úÖ Strategic injection target reached: {injection_accuracy:.1f}%")
                        break
                
                print(f"‚úÖ Enhanced strategic injection completed for Stage {stage}")
            except Exception as e:
                print(f"‚ö†Ô∏è Strategic injection failed: {e}")
        
        # Stage training loop with enhanced features
        stage_epochs = MINERVA_CONFIG['epochs_per_stage']
        stage_best_exact = 0.0
        
        for epoch in range(stage_epochs):
            global_epoch += 1
            
            # Training phase with enhancements
            model.train()
            train_metrics = defaultdict(float)
            
            pbar = tqdm(train_loader, desc=f"MINERVA V3 Stage {stage}, Epoch {epoch+1}")
            
            for batch_idx, batch in enumerate(pbar):
                inputs = batch['inputs'].to(device, non_blocking=True)
                outputs = batch['outputs'].to(device, non_blocking=True)
                
                # Clamp values
                inputs = torch.clamp(inputs, 0, 9)
                outputs = torch.clamp(outputs, 0, 9)
                
                # Advanced strategic augmentation
                if MINERVA_CONFIG.get('advanced_augmentation') and random.random() < 0.35:
                    inputs, outputs = advanced_strategic_augmentation(inputs, outputs)
                
                # Convert to one-hot
                input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                
                # Apply mixup augmentation
                mixup_lambda = None
                if random.random() < 0.25:  # 25% chance of mixup
                    input_grids, output_targets, mixup_lambda = mixup_data(
                        input_grids, output_grids, alpha=0.3
                    )
                    output_grids = output_targets
                
                with autocast(device.type):
                    model_outputs = model(input_grids, mode='train')
                    losses = loss_fn(model_outputs, output_grids, input_grids, mixup_lambda)
                
                loss = losses['total'] / MINERVA_CONFIG['gradient_accumulation']
                scaler.scale(loss).backward()
                
                if (batch_idx + 1) % MINERVA_CONFIG['gradient_accumulation'] == 0:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), MINERVA_CONFIG['gradient_clip'])
                    scaler.step(optimizer)
                    scaler.update()
                    optimizer.zero_grad()
                    scheduler.step()
                
                # Update metrics
                train_metrics['loss'] += losses['total'].item()
                train_metrics['exact'] += losses['exact_count'].item()
                train_metrics['ultra_teal'] += losses['ultra_teal_count'].item()
                train_metrics['samples'] += inputs.size(0)
                
                # Enhanced progress display
                pbar.set_postfix({
                    'loss': f"{losses['total'].item():.3f}",
                    'exact': f"{losses['exact_count'].item():.0f}",
                    'teal': f"{losses['ultra_teal_count'].item():.1f}",
                    'IoU': f"{losses.get('avg_iou', torch.tensor(0)).item():.2f}",
                    'strategic': f"{losses.get('strategic_bonus', torch.tensor(0)).item():.3f}",
                    'lr': f"{scheduler.get_last_lr()[0]:.6f}"
                })
            
            # Enhanced validation every 5 epochs
            if epoch % 5 == 0 or epoch == stage_epochs - 1:
                model.eval()
                val_metrics = defaultdict(float)
                
                with torch.no_grad():
                    for batch in tqdm(val_loader, desc="V3 Enhanced Validation"):
                        inputs = batch['inputs'].to(device)
                        outputs = batch['outputs'].to(device)
                        
                        inputs = torch.clamp(inputs, 0, 9)
                        outputs = torch.clamp(outputs, 0, 9)
                        
                        input_grids = F.one_hot(inputs, num_classes=10).permute(0, 3, 1, 2).float()
                        output_grids = F.one_hot(outputs, num_classes=10).permute(0, 3, 1, 2).float()
                        
                        with autocast(device.type):
                            model_outputs = model(input_grids, mode='inference')
                            losses = loss_fn(model_outputs, output_grids, input_grids)
                        
                        val_metrics['loss'] += losses['total'].item()
                        val_metrics['exact'] += losses['exact_count'].item()
                        val_metrics['ultra_teal'] += losses['ultra_teal_count'].item()
                        val_metrics['samples'] += inputs.size(0)
                
                # Calculate and display enhanced metrics
                train_exact_pct = train_metrics['exact'] / train_metrics['samples'] * 100
                train_ultra_teal_pct = train_metrics['ultra_teal'] / train_metrics['samples'] * 100
                train_loss = train_metrics['loss'] / len(train_loader)
                val_exact_pct = val_metrics['exact'] / val_metrics['samples'] * 100
                val_ultra_teal_pct = val_metrics['ultra_teal'] / val_metrics['samples'] * 100
                val_loss = val_metrics['loss'] / len(val_loader)
                
                print(f"\nüß† MINERVA V3 Stage {stage}, Epoch {epoch+1} (Global: {global_epoch}):")
                print(f"   üéØ Train: {train_exact_pct:.2f}% exact | {train_ultra_teal_pct:.2f}% ULTRA TEAL | Loss: {train_loss:.3f}")
                print(f"   üéØ Val: {val_exact_pct:.2f}% exact | {val_ultra_teal_pct:.2f}% ULTRA TEAL | Loss: {val_loss:.3f}")
                print(f"   üìä LR: {scheduler.get_last_lr()[0]:.6f} | Grid: {grid_size}x{grid_size} | Complexity: {pattern_complexity}")
                
                # Track stage best
                if val_exact_pct > stage_best_exact:
                    stage_best_exact = val_exact_pct
                
                # Save enhanced best model
                if val_exact_pct > best_exact:
                    best_exact = val_exact_pct
                    torch.save({
                        'epoch': global_epoch,
                        'stage': stage,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'best_exact': best_exact,
                        'config': MINERVA_CONFIG,
                        'stage_config': STAGE_CONFIG
                    }, best_model_path)
                    print(f"   üíæ NEW V3 BEST: {val_exact_pct:.2f}% exact match saved!")
        
        # Store stage results
        stage_results[stage] = {
            'grid_size': f"{grid_size}x{grid_size}",
            'pattern_complexity': pattern_complexity,
            'best_exact': stage_best_exact,
            'final_epoch': global_epoch
        }
        
        print(f"\nüß† Stage {stage} complete! Best: {stage_best_exact:.2f}% | Complexity: {pattern_complexity}")
    
    # Final results summary
    print(f"\nüéâ MINERVA V3 Enhanced Strategic Training Complete!")
    print("=" * 60)
    print(f"   üèÜ Best exact match: {best_exact:.2f}%")
    print(f"   üìè Enhanced stages completed: 10 (6x6 ‚Üí 35x35 grids)")
    print(f"   üìä Total epochs: {global_epoch}")
    print(f"   üß† Enhanced with program synthesis and strategic reasoning")
    
    print(f"\nüìè Stage-by-stage Enhanced Learning Progression:")
    for stage, results in stage_results.items():
        print(f"   Stage {stage} ({results['grid_size']}): {results['best_exact']:.2f}% | {results['pattern_complexity']}")
    
    return model, best_exact


if __name__ == "__main__":
    print("üöÄ Starting MINERVA V3 Enhanced Strategic Training...")
    model, best_performance = train_minerva_specialized_v3()
    print("‚úÖ MINERVA V3 training completed successfully!")
    print(f"üß† Final Enhanced Performance: {best_performance:.2f}% exact match")